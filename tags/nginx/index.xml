<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NGINX on Jing Lin‘s profile</title><link>http://linjing.io/tags/nginx/</link><description>Recent content in NGINX on Jing Lin‘s profile</description><generator>Source Themes academia (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright &amp;copy; {year} linjing.io host on github, imesh.cloud host on netlify. CI/CD by github actions and netlify.Thanks bootcdn support for front libary CDN</copyright><lastBuildDate>Thu, 01 Dec 2022 15:10:30 +0000</lastBuildDate><atom:link href="http://linjing.io/tags/nginx/index.xml" rel="self" type="application/rss+xml"/><item><title>NGINX Gateway API</title><link>http://linjing.io/talk/nginx-sprint-gateway-api/</link><pubDate>Thu, 01 Dec 2022 15:10:30 +0000</pubDate><guid>http://linjing.io/talk/nginx-sprint-gateway-api/</guid><description>&lt;!-- raw HTML omitted --></description></item><item><title>Analysis and Thinking of Enterprise Open Source</title><link>http://linjing.io/post/analysis-and-thinking-of-enterprise-opensource/</link><pubDate>Mon, 27 Jun 2022 00:00:00 +0000</pubDate><guid>http://linjing.io/post/analysis-and-thinking-of-enterprise-opensource/</guid><description>&lt;blockquote>
&lt;p>In recent years, the role of open source software in promoting the digital transformation of enterprises has become more and more obvious. The country has written the construction and improvement of the open source ecosystem into the &amp;ldquo;14th Five-Year Plan&amp;rdquo;. At the same time, topics such as open source governance and secure supply chain have become a hot topic in the industry. As open source users or as open source subjects, how should we understand open source, how to understand the community, and how to participate in open source better and more safely. This article will discuss with you the meaning of open source, business models, risks and challenges, choices and governance.&lt;/p>
&lt;/blockquote>
&lt;h3 id="what-open-source-means-for-business">What Open Source Means for Business&lt;/h3>
&lt;p>Let us first think about the meaning of open source and individuality from the perspective of an individual. We can summarize &amp;ldquo;selfishness and altruism&amp;rdquo; in one sentence. Using open source projects or products can help individuals quickly solve problems and challenges at work, and individuals can also gain innovative ideas and experience from open source projects or products. These are all selfish. When an individual is not satisfied with just being an open source user, and begins to participate in the contribution of an open source project, or even set up an open source project himself, it becomes altruistic, and other users will benefit from his contribution or project.&lt;/p>
&lt;p>The same goes for businesses. When enterprises focus on their own digital transformation, it means that enterprises will pursue a more open organizational spirit. The core of digital transformation is to use digital technology and capabilities to reengineer enterprise processes, thereby driving business model innovation, enhancing enterprise competitiveness, and even changing an industry, just like &amp;ldquo;DiDi&amp;rdquo;. This requires the enterprise&amp;rsquo;s IT infrastructure and technology to not only support the rapid changes and development of the business, but also drive business innovation. Enterprises must quickly establish business, quickly iterate business, and quickly obtain business feedback. Therefore, having an IT architecture and technology that can flexibly support short-term and long-term business development is the key. Enterprises must adopt open and collaborative new technologies to continuously improve and enhance the iterative capability of technical architecture. These all mean being &amp;ldquo;fast&amp;rdquo; enough and &amp;ldquo;innovative&amp;rdquo; enough. Open source technology just responds to the needs of enterprises, because open source technology pursues the spirit of openness, collaboration and contribution. Enterprises adopting open source technology can obtain contributions from outstanding talents from all over the world, and can rapidly improve and enhance the competitiveness of enterprises through the contributions of talents from all over the world. The open ecosystem created by open source brings this value to all parties involved.&lt;/p>
&lt;p>Take the smart car industry as an example. Traditionally, car manufacturers have relied on a large number of third-party suppliers and technologies. However, when companies enter the new smart car track, car companies need to continuously innovate vehicle functions, improve user experience, and quickly occupy the market. If car companies still adopt the traditional supplier model, they will be constrained by those closed suppliers and will not be able to innovate products and quickly occupy the market. By adopting open source technologies, such as AGL (Automotive Grade Linux), it is possible to eliminate the need to develop the basic operating system of the vehicle, and instead invest valuable R&amp;amp;D technology on more competitive innovations.&lt;/p>
&lt;p>Enterprises with an open spirit will not only stop at adopting open source technologies, but will also contribute to open source. Large enterprises can help enterprises realize market strategies, occupy the market or change the game rules of the market by open-sourcing their own products or technologies, such as Google&amp;rsquo;s Android system. Innovative small enterprises can accumulate huge community users, increase their influence, and expand their own valuations through open source.&lt;/p>
&lt;p>It can be seen that the significance of open source to the digital transformation of enterprises is obvious. On the one hand, open source is conducive to stimulating technological innovation of enterprises. Compared with the use of closed source software, open source technology makes users more creative and innovative; on the other hand, the use of open source technology can help enterprises save costs, so that more IT investments are used to deploy new technologies and accelerate digital transformation. This finding is echoed in a survey of global IT leaders on the state of open source in the enterprise 2020, with 95% of respondents saying open source is strategically important to their overall software strategy. In China, some leading Internet companies have also established special corporate open source committees to help companies achieve better open source strategies. The country has written the construction and improvement of the open source ecosystem into the &amp;ldquo;14th Five-Year Plan&amp;rdquo;.&lt;/p>
&lt;p>Open source is sometimes given the concept of &amp;ldquo;trusted and controllable&amp;rdquo;. It is true that open source cannot be directly equated with &amp;ldquo;trustworthy and controllable&amp;rdquo;. Only when an enterprise has the ability and resources to thoroughly understand and master it through learning can it be truly trusted and controllable. However, open source, in the form of open source code, allows users to be reassured and solve problems in operation and maintenance by studying open source code when necessary.&lt;/p>
&lt;p>F5 operates open source NGINX in China and deeply understands the demands of Chinese users. The reliability and ease of development of NGINX technology ensure that enterprises can have a better and more reliable IT infrastructure, which is of great significance to the digital transformation of enterprises. Whether in distributed or microservice architecture, NGINX technology has been proven by users all over the world, and its reliability ensures the stability of IT infrastructure. In China, we can see that a large number of users use NGINX technology in production, whether it is an Internet company or a financial enterprise, such as the open banking of Agricultural Bank, the business access scheduling of Xinwang Bank, and so on. The ease of development of NGINX makes these technological innovations possible. It can be said that many customers have realized the autonomous control of soft loads with the help of NGINX. Further, by localizing open source NGINX and providing open source subscription services in China, it means that Chinese users can achieve more reliable product use through local services. This service includes basic technical support and advanced expert services. Creation, development, etc.&lt;/p>
&lt;h3 id="open-source-business-model">Open source business model&lt;/h3>
&lt;p>There are many business models for open source. According to the origin, background, appeal, and subject of the project, it can generally be divided into:
advertising mode, which can obtain certain benefits by placing sponsored advertisements on the website, installation process, documents, etc. Games and search projects are generally easy to take this approach.&lt;/p>
&lt;p>&lt;strong>Selling services&lt;/strong> is a typical open source business model. The service provider itself does not provide code sales, but services, and users pay for better and more professional technical support services. The sale of services can be the original subject of the open source project or other subjects. Of course, it is generally difficult to support the development of a company by simply selling services. Generally speaking, companies will adopt a mixed business model, such as Redhat.&lt;/p>
&lt;p>&lt;strong>Software redistribution&lt;/strong> , enterprises expand and enrich upstream open source projects through their own development, and form commercial products for redistribution and sales. This is a relatively common approach, such as there is a large number of commercial distributions or reintegration of enterprise products around kubernetes.&lt;/p>
&lt;p>&lt;strong>Direct code sale&lt;/strong> , the definition of open source allows you to sell source code directly. In the early days of the Internet, revenue was made by collecting, distributing code or binary software and making it on CD-ROM. Relying on direct sales codes is obviously not easy to succeed today.&lt;/p>
&lt;p>&lt;strong>Dual license/open core&lt;/strong> , the main body of the open source project has a commercial license version of the software while publishing the source code. Generally speaking, there will be some functional differences between the two. Open source products will have the main core functionality, but some additional functionality needed for production-level enterprise deployments will be sold in commercial versions. Such companies often also sell additional services at the same time. NGINX Open Source and NGINX Plus are a dual license. NGINX Plus is completely based on open source OSS, but has added many enterprise-level features, so that users can better deploy and run in the production environment.&lt;/p>
&lt;p>&lt;strong>SaaS&lt;/strong> , deploying open source software as a SaaS service model on the cloud, is also a popular way at present. It is also a model in which open source products are more likely to achieve commercial success in the cloud era. MongoDB&amp;rsquo;s SaaS service is this model. Of course, some open source controversies have arisen because of cloud services, which will be discussed later in the Risks and Challenges section.&lt;/p>
&lt;p>&lt;strong>Eco-partnership&lt;/strong> , commercialization in the form of an eco-partner is a hybrid business approach. Some companies have full-time employees involved in some very well-known and popular open source projects, such as Istio, Kubernetes, etc. Contribute to these upstream projects and enter the technical committees, SIGs, etc. of these upstream projects, forming a high influence in the community and field. The company itself will sell related professional service support, value-added products, etc. This is a more advanced form of business, generally found in large companies or very innovative start-ups. Companies such as Solo and Tetrate. F5 NGINX&amp;rsquo;s contribution to the community k8s Ingress Controller is also in this pattern.&lt;/p>
&lt;p>There are many open source business models, such as donations, crowdfunding, and peripheral brands. Either way, from the user&amp;rsquo;s perspective, the product must have value in order to be paid for. For companies that rely on open source for commercial services, they must provide valuable products and services with good experience, and return to the origin of the product to achieve open source commercialization more easily. It is difficult to achieve sustainable open source commercialization only by relying on the market.&lt;/p>
&lt;h3 id="open-source-risks-and-challenges">Open Source Risks and Challenges&lt;/h3>
&lt;p>Open source has many benefits for businesses or users. However, in the actual use process, there will still be certain risks and challenges. &lt;strong>From the risk point of view&lt;/strong> , there are the following four aspects:
&lt;strong>License risk&lt;/strong> , which may be the first consideration in the use of open source. Generally speaking, free software licenses are in copyleft mode. Such licenses are generally stricter and will force downstream users to keep open source. Therefore, be careful whether you will violate the license requirements after modifying the code. For example, the GPL License of HAproxy should be handled carefully. This is a typical copyleft license. When distributing the modified and compiled binary, you must attach the modified source code. Packaged products with delivery capabilities implemented using HAproxy also need a short copyright statement at the delivery prompt. Although some open source software licenses today no longer require users to open source the modified code, it is still necessary to be careful about the license scenarios and restrictive requirements. For example, whether open source code is used in prohibited usage scenarios, Redis&amp;rsquo;s RSAL restricts usage scenarios such as search engines and stream processing engines. When using open source software to build cloud service capabilities, you must be careful about open source products using the Affero GPL or SSPL protocol. He requires the relevant source code to be disclosed, which is very unfriendly to many companies similar to public clouds, so Google strictly refuses it internally. Use AGPL&amp;rsquo;s software on the public cloud.&lt;/p>
&lt;p>&lt;strong>Project continuity risk&lt;/strong> , due to the uneven main body of open source, some projects are the results of some enterprise KPI-oriented, and some are the results of individuals based on hobbies or work stages (it can be called: open source smoothly). These items may not be durable and have a short life cycle. If an enterprise chooses such a project, it must fully understand such risks, and the enterprise needs to be able to develop and maintain it continuously. Persistent risks may also be manifested in some geopolitical issues. Although the Open Source Initiative (OSI) mentions &amp;ldquo;non-discrimination against individuals or groups&amp;rdquo; in its 10 definitions of open source, such concerns may still be encountered under certain circumstances. Sometimes these concerns are based on personal emotions or based on personal perception. For example, some time ago, F5&amp;rsquo;s statement on stopping the work of F5&amp;rsquo;s Russian office staff made it emotionally unacceptable to a small number of people, misunderstanding that NGINX originated in Russia, but F5 stopped Russians&amp;rsquo; contributions to NGINX. In fact, this is just a decision made by F5 based on the regional war for the safety of F5&amp;rsquo;s internal staff. The source code of NGINX has been hosted on github and mercurial multiple service systems, and contributions, access, and downloads from Internet users around the world have not been affected.&lt;/p>
&lt;p>&lt;strong>Project quality risk&lt;/strong> , open source projects pursue openness, and developers&amp;rsquo; experience levels are not completely consistent, which may lead to insufficient code testing, code security risks, and incomplete use cases. Some large and popular open source projects often improve software quality by standardizing developer contributions, setting up special testers, documentation personnel, and security teams. But not all projects have such resources and capabilities. Enterprises should do sufficient research and testing when introducing open source projects.&lt;/p>
&lt;p>&lt;strong>Open source nesting risks&lt;/strong> , such risks need to be considered from two perspectives. One is the license. When citing other open source projects in your own project, you should pay attention to the relationship between the reference method and the license, and pay attention to whether there is a compatibility risk between the license of your own project and the license of the referenced project. The second is to pay attention to the loss of quality control caused by serial nesting. When project A cites B, and B cites C and other multi-level references, it is necessary to comprehensively judge all the above-mentioned risks.&lt;/p>
&lt;p>&lt;strong>From a challenge perspective&lt;/strong> , its coverage is much broader. Generally speaking, the digital transformation of enterprises involves three aspects: culture, technology and process. Likewise, the challenges of enterprise use of open source can also be considered from these three perspectives.
Culturally, the lack of motivation for companies to build an open source culture is a challenge. Enterprises often emphasize the stability of operation. Whether it is a process or KPI assessment, stability is often pursued for IT systems. From the bidding, testing, launching, and operation and maintenance of the project, stability is the first. In such a culture, the technical system and personnel ideas tend to become rigid: the almost static and stable IT system limits the IT system’s support for business innovation, resulting in slow business launch; technical personnel will rely on the technical support of commercial products, resulting in a lack of innovation Spirit. On the other hand, the lack of motivation for open source culture is that enterprises lack the spirit of open source contribution. They only ask for use, do not contribute to the upstream of open source, and are even more afraid to share their innovations with the community.&lt;/p>
&lt;p>&lt;strong>On the technical side&lt;/strong> , there are two interrelated challenges: the ability to master open source technology, and the talent to master the technology. Open source products are often not ready to use out of the box. Enterprises need to re-develop according to their own conditions. At the same time, in order to achieve business capabilities, they often need to adopt a variety of open source technologies. This requires enterprises to have more development talents and more innovative talents, who need these talents to analyze and study these open source products, understand and master core technologies. This is often a big challenge for companies that adopt outsourcing models, and such companies lack enough talents to master these technologies. Even for enterprises with a certain development scale, how to transform talents, establish a good talent promotion channel, and retain high-end talents who master key open source technologies and understand open source technologies are actually very big challenges.&lt;/p>
&lt;p>&lt;strong>In terms of processes&lt;/strong> , companies need to adapt existing processes to open source, which is often difficult. Using open source technology means that processes oriented towards closed source software need to be transformed and adapted to the changes brought about by open source technology, whether it is business processes, asset management, technical support, governance processes, etc. We have seen that some enterprises still encounter some problems in the process of purchasing open source-related technical support services after using open source.&lt;/p>
&lt;h3 id="open-source-selection-and-governance">Open Source Selection and Governance&lt;/h3>
&lt;p>Open source subjects and open source users think about open source selection and governance in two different categories. Because it&amp;rsquo;s a question of two directions, even though the roles of the two sometimes intersect.&lt;/p>
&lt;p>&lt;strong>Open source choice, for open source subjects&lt;/strong> , the first thing to do is to make a decision on whether or not to open source their projects. The person in charge of the project or company needs to think about it based on the cognitive understanding of open source, combined with the characteristics of the industry, the analysis of competition in the field, and its own actual situation, and comprehensively analyze the impact of open source or not open source. When you decide to open source, you also need to decide whether to open source internally or externally. Intra-enterprise open source is a practice of some leading Internet or technology-leading large enterprises. Generally speaking, the internal open source of an enterprise will go through the spontaneous stage of individuals or groups, and then to the stage of unified open source management and coordination at the global level of the enterprise by setting up an internal open source management department in the enterprise.&lt;/p>
&lt;p>When the official decision to open source is made, then it is necessary to turn to thinking about open source governance. For open source entities, open source governance includes two aspects, one is the governance of project engineering, and the other is community governance.&lt;/p>
&lt;p>(1) Project engineering governance. In any case, an open source project is ultimately a software engineering, but it is based on a larger scope of collaboration, trust and contribution. Therefore, the management of project engineering quality and code is also applicable to open source projects. It is necessary to consider the scope management, schedule management, quality management, change control, etc. of the project, and ensure the quality, progress, and safety of the code through these managements. We can see that some open source projects will give developer guidance, Code of conduct, etc., which are used to ensure code quality. At the same time, it is also necessary to build a good DevOps automation pipeline to make the contribution process of developers smoother, to ensure that the submitted code undergoes relevant reviews, automated inspections, automated tests, etc., and the results are fed back to contributors in a timely manner. In addition, risk management needs to be done well. We can see that many mature open source projects require developers to sign a Developer Contribution Agreement (CLA) before submitting PR. These are to prevent copyright, non-original and other risks. Risk management also includes license compliance management for other open source components used in the project to avoid introducing risks. Tools like FOSSA are often used for this management.&lt;/p>
&lt;p>(2) Project community governance. The biggest feature of open source projects is decentralization and diverse personnel. They come from all over the world, with different backgrounds and cultures. Therefore, the governance of the community is essentially the management of &amp;ldquo;stakeholders&amp;rdquo;, and the management of people is the biggest challenge in open source projects. It can be said that the quality of community governance is a very critical factor for the success of the project, so the Apache Foundation particularly emphasizes the concept of &amp;ldquo;Community over code&amp;rdquo;. A community should have clear rules and a tone from the start, which ensures that the community always brings together those who share the same understanding. The governance of the community will involve choosing which governance model, such as foundation custody or self-management. Generally speaking, joining the foundation is beneficial to the operation of the project, because these professional open source foundations can help guide the operation of the project, and can also help the project go global quickly, help build the project ecology, and form a relationship between different projects through the power of the foundation. cross support. Of course, the foundation also runs many activities, which can help the project to increase its visibility, attract more developers to become users, and eventually become contributors. Self-management requires the project owner or the organization behind it to have strong community management capabilities. For example, NGINX is self-managed, and F5 manages the community through a professional community operation team. No matter what kind of governance method, the core is to help the project walk on the right track and direction, ensure the sustainable development of the project, and solve various stakeholder problems in the process of the project. The work required for community governance can include product evangelism, event operation, developer relationship maintenance, Issues/PR management, license management, community contract and rule management, document management, ecological construction, legal affairs, etc.&lt;/p>
&lt;p>&lt;strong>Open source selection. For open source users&lt;/strong> , especially for enterprise users, the first rule of open source selection is to establish access control. Enterprises can consider establishing a whitelist of open source software assets to prevent developers from randomly introducing open source software or projects. The software provided by the purchased service provider is also subject to relevant whitelist checks. Although this increases the cost to a certain extent, it is indeed very necessary for the enterprise&amp;rsquo;s security risk control. Enterprises should conduct sufficient research and analysis on open source projects to understand the status, activity, supporting force behind the project, operation mode, user base, contributor popularity, license restrictions, technical roadshows, software architecture, code quality, etc., and do a good job Adequate pre-introduction testing. Open source projects should also be introduced objectively to avoid the introduction of &amp;ldquo;relational open source projects&amp;rdquo; due to the technical feelings or inclinations of a small number of people.&lt;/p>
&lt;p>Let&amp;rsquo;s look at &lt;strong>open source governance&lt;/strong> . As with the challenges of using open source above, enterprises can conduct open source governance in terms of culture, technology, and process.&lt;/p>
&lt;p>(1) In terms of culture, enterprises should establish a culture of advocating and respecting open source. While actively encouraging the adoption of open source technologies, strengthen employee awareness of open source. Such as respecting copyright and avoiding legal risks. Knowing open source does not mean free, and using open source does not mean cheating. Knowing open source doesn&amp;rsquo;t mean you can be autonomous and controllable. Open source does not mean customization, and any modifications and enhancements that are beneficial to the product should be fed back to the upstream. Enterprises should objectively evaluate their current ability to control open source according to their own actual situation, and should not rashly advance. For example, enterprises may need to shape open source cultural genes step by step. At this time, it is more suitable for enterprises to adopt open source software usage models with professional support services. By introducing the support of third parties or open source service providers to help enterprises avoid technical risks and achieve pragmatic independent availability control. Taking soft load products as an example, enterprises can try open source practices by introducing NGINX support services. For departments that have just undergone open source practice transformation, such as operation and maintenance departments, they can consider adopting commercial products + manufacturers&amp;rsquo; open source expansion solutions to ensure that they gradually enter open source operation and maintenance under the premise of controllable risks.&lt;/p>
&lt;p>(2) In terms of technology, establish a good development construction and testing platform and security testing platform in the development link, and conduct code scanning and inspection on relevant open source codes. Identify dependencies of related libraries and discover potential vulnerabilities in the code itself and associated dependencies. Check for possible license compliance issues and cross-problems through open source license management software. During delivery and operation, use additional security devices or strategies to strengthen the environment in which open source components run reinforcement. Strengthen the technical team&amp;rsquo;s learning and skill improvement of open source technology, and establish a professional open source technology component support team.&lt;/p>
&lt;p>(3) In terms of process, enterprises can consider establishing a whole process mechanism from introduction, development, delivery, operation and maintenance to exit for open source management. From the aspects of organizational mechanism and management system, the open source software introduction specification, development specification, deployment specification, operation and maintenance specification, exit management and other specifications are formed. The introduction of specifications can be combined with the above-mentioned open source selection part to identify open source access, establish access conditions, and make the first pass of the entrance. The development specification can consider defining the use language, paradigm, boundary, modification process, documentation process, etc. of open source software code. Deployment specifications can be considered around delivery, dependency management, security hardening, and standardized environments. Operation and maintenance specifications can consider the operation and maintenance tools, troubleshooting processes, best practices and other aspects of open source software. In addition, enterprises should also form a closed-loop management system, and establish an identification and inspection mechanism for the use and operation of open source. For example, identify open source software and related projects that are already running, and evaluate them, make corrections in terms of culture, process, and technology for the problems found, and exit open source software that is not running well in time to ensure that the governance of open source is always Stay on track to avoid open source sprawl and runaway.&lt;/p>
&lt;h3 id="summarize">Summarize&lt;/h3>
&lt;p>It can be seen that, whether it is an open source subject or an open source user, the understanding, choice, risk, challenge, and governance of open source are all systematic projects and capabilities. In recent years, under the strategic background of the country&amp;rsquo;s vigorous promotion of open source, the concept of &amp;ldquo;trusted open source&amp;rdquo; has been proposed in China, and its main goal is still how to better play the role of open source and avoid possible risks in open source. In any case, if an open source project can always adhere to its original intention and insist on being user-centered, it can eliminate users&amp;rsquo; concerns and risks in open source in many aspects. As Zhang Yiqiang, general manager of F5 China, said at the &amp;ldquo;2022 F5 Multi-Cloud Application Service Technology Summit&amp;rdquo;: Focus on user needs and build an open source ecosystem around product features and community platforms. This is the core that ensures that NGINX can go a long way in China, the core that F5 provides value to users, and the core that users trust F5. I think the same applies to our understanding of other open source.&lt;/p></description></item><item><title>New NGINX book published!</title><link>http://linjing.io/post/nginx-book/</link><pubDate>Mon, 16 May 2022 00:00:00 +0000</pubDate><guid>http://linjing.io/post/nginx-book/</guid><description>&lt;h3 id="this-is-the-first-nginx-book-after-f5-acquiring-nginx-it-not-only-just-focus-on-how-to-use-nginx-directives-it-helps-you-to-understand-nginx-by-vary-scenarios">This is the first NGINX book after F5 acquiring NGINX. It not only just focus on how to use NGINX directives, It helps you to understand NGINX by vary scenarios.&lt;/h3>
&lt;p>Pls order it in jd.com, Chinese book name： NGINX经典教程&lt;/p>
&lt;p>&lt;img src="img/image-20220516125953377.png" alt="image-20220516125953377">&lt;/p>
&lt;p>&lt;img src="img/image-20220516130010458.png" alt="image-20220516130010458">&lt;/p>
&lt;p>&lt;img src="img/image-20220516130043488.png" alt="image-20220516130043488">&lt;/p>
&lt;p>&lt;img src="img/image-20220516124855457.png" alt="image-20220516124855457">&lt;/p></description></item><item><title>NGINX Classic Tutorial</title><link>http://linjing.io/publication/nginx-book-publish/</link><pubDate>Mon, 16 May 2022 00:00:00 +0000</pubDate><guid>http://linjing.io/publication/nginx-book-publish/</guid><description>&lt;!-- raw HTML omitted --></description></item><item><title>How an application delivery veteran see Envoy in the era of cloud native</title><link>http://linjing.io/post/f5-envoy-cloud-native/</link><pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate><guid>http://linjing.io/post/f5-envoy-cloud-native/</guid><description>&lt;h2 id="foreword">Foreword&lt;/h2>
&lt;p>Envoy, messenger, envoy, representative! Just like the meaning of the word itself, with a sense of authority, a sense of sacred full agent. Combined with its own use and role, it is really &amp;ldquo;people as their name&amp;rdquo;, can&amp;rsquo;t help but like Lyft, I don&amp;rsquo;t know which master got the name to get this name. In the current era of fiery microservices, Envoy is an absolute star, and it is no exaggeration to describe it as everyone knows. Someone once asked me how to look at Envoy and whether Envoy will replace F5 instead of NGINX in the cloud-native era. As a veteran who has experienced two waves of change in the field of application delivery technology, in this article I will talk about Envoy in the future From a perspective to understand and answer this question. Why talk a little bit, this is really not modesty, but objectively, there is really no such in-depth large-scale long-term use and research of all technical details of Envoy, so I will combine my professional experience and experience to make an Envoy Talk.&lt;/p>
&lt;h2 id="star-studded-envoy">Star-studded Envoy&lt;/h2>
&lt;p>First, let&amp;rsquo;s take a look at how Envoy officially introduced Envoy:&lt;/p>
&lt;blockquote>
&lt;p>ENVOY IS AN OPEN SOURCE EDGE AND SERVICE PROXY, DESIGNED FOR CLOUD-NATIVE APPLICATIONS&lt;/p>
&lt;/blockquote>
&lt;p>From this description on the homepage of the website, we can clearly see the official definition of Envoy, which is simply a proxy for east-west, north-south traffic in the cloud native era. Lfyt is the pioneer of the microservice application architecture. We can see Lfyt in a large number of microservice sermon articles. After a large-scale shift from monolithic applications to microservice architecture, a serious problem was placed in development. In front of the architects, on the one hand, Lyft&amp;rsquo;s services are developed in multiple languages, and the use of class libraries to solve various problems under the distributed architecture requires a lot of language adaptation and code intrusion. On the other hand, Lyft&amp;rsquo;s business Both are deployed on AWS, relying heavily on AWS&amp;rsquo; ELB and EC2, but the traffic control, insight, and problem elimination between the services provided by ELB and AWS at that time could not meet Lyft&amp;rsquo;s needs. It is based on this background, Lfyt is Envoy development started in May 2015. It was first deployed as an edge agent and began to replace ELB, and then began to be deployed as a sidecar method for large-scale deployment. On September 14, 2016, Lyft officially announced this project on its blog: Envoy C++ L7 proxy and communication bus . For a while, Envoy received a lot of attention, and companies such as Google began to contribute to this project, and donated the project to CNCF one year later in September 2017. With a good mom like Lyft, and the succession to CNCF as a rich dad, plus the half-brother Istio star brother&amp;rsquo;s blessing, it can be said that Envoy has a good scene for a while, earning enough eyeball and developer support, I graduated from CNCF in just over a year.&lt;/p>
&lt;p>Container technology has helped enterprises practice Devops and microservice transformation. The k8s container orchestration platform allows enterprises to move more business from traditional architectures to modern container-based infrastructures with more confidence. k8s solves container orchestration and applications. Issues such as publishing, but when the communication between services has changed from the previous call between memory to TCP-based network communication, the impact of the network on application services has become more huge and uncertain, based on traditional application architecture operation and maintenance The means cannot adapt and solve the huge and complex communication insights and troubleshooting between services. In order to solve such problems, the service mesh application was born and quickly became a hot topic of concern. The Istio project is the most important player in this ecosystem. Istio&amp;rsquo;s architecture is a typical management plane and data separation architecture. The choice of data plane is open, but Istio chooses Envoy as the data plane by default. The two popular stars joined forces to make Linkerd eclipsed almost at the same time. At this point in time, NGINX also briefly carried out the Nginmesh project, trying to make NGINX as the data plane of Istio, but eventually gave up at the end of 2018, why did you give up, this article will be mentioned later.&lt;/p>
&lt;p>In addition to Istio&amp;rsquo;s selection of Envoy as the data plane, there are many projects based on Envoy, such as multiple Ingress Controller projects of k8s: Gloo, Contur, Ambassador. Istio&amp;rsquo;s own Ingress gateway and Egress gateway also choose Envoy. Take a look at the Envoy users listed on their official homepage and say that starlight is not too much. Note that F5 in the list is very interesting.&lt;/p>
&lt;p>&lt;img src="envoy-endusers.jpg" alt="">
(Envoy end user list)&lt;/p>
&lt;h2 id="envoy-born-for-the-times">Envoy: born for the times&lt;/h2>
&lt;p>Below I will look at the technical aspects of why Envoy is so valued by the community. It will be summarized from the following aspects:&lt;/p>
&lt;ul>
&lt;li>Technical characteristics&lt;/li>
&lt;li>Deployment architecture&lt;/li>
&lt;li>Software Architecture&lt;/li>
&lt;/ul>
&lt;h3 id="technical-characteristics">Technical characteristics&lt;/h3>
&lt;ul>
&lt;li>Interface and API&lt;/li>
&lt;li>Dynamic&lt;/li>
&lt;li>Scalability&lt;/li>
&lt;li>Observability&lt;/li>
&lt;li>Modernity&lt;/li>
&lt;/ul>
&lt;h4 id="interface-and-api">Interface and API&lt;/h4>
&lt;p>When I first opened the configuration of Envoy, my first feeling was, God, how should such a product user configure and use. Under the intuitive experience, in an uncomplicated experimental environment, the number of lines of an Envoy&amp;rsquo;s actual configuration file actually reached 20,000 lines.&lt;/p>
&lt;pre>&lt;code># kubectl exec -it productpage-v1-7f4cc988c6-qxqjs -n istio-bookinfo -c istio-proxy -- sh
$ curl http://127.0.0.1:15000/config_dump | wc -l
% Total % Received % Xferd Average Speed Time Time Time Current
Dload Upload Total Spent Left Speed
100 634k 0 634k 0 0 10.1M 0 --:--:-- --:--:-- --:--:-- 10.1M
20550
&lt;/code>&lt;/pre>&lt;p>Although this is a dynamic configuration in the Istio environment, although there are ways to optimize it to reduce the actual configuration amount, or that we will not do such a large amount of configuration when using the static configuration method for configuration, but when we see the following actual The configuration structure output will feel that for such a software, it is obviously impractical to configure and maintain in a normal way. Its configuration is completely json structured and has a large number of descriptive configurations. Compared to NGINX and other such reverse For agent software, its configuration structure is too complicated.&lt;/p>
&lt;p>&lt;img src="envoy-json.jpg" alt="">
(Complex configuration structure)&lt;/p>
&lt;p>Obviously, Envoy&amp;rsquo;s design is not designed for manual, so Envoy designed a large number of xDS protocol interfaces, users need to design an xDS server to implement all configuration processing, Envoy supports gRPC or REST to communicate with the server to update Own configuration. xDS is the general name of the Envoy DS (discover service) protocol, which can be divided into Listener DS (LDS), Route DS (RDS), Cluster DS (CDS), Endpoint DS (EDS), and Secret DS in order to ensure consistent configuration DS-ADS of polymerization and the like, may be more xDS view here . These interfaces are used to automatically generate various specific configuration objects. It can be seen that this is a highly dynamic runtime configuration. To use it well, you must develop a server with sufficient capabilities. Obviously this is not the design thinking of traditional reverse proxy software.&lt;/p>
&lt;p>&lt;img src="envoy-xds.png" alt="">
(Picture from &lt;a href="https://gist.github.com/nikhilsuvarna/bd0aa0ef01880270c13d145c61a4af22">https://gist.github.com/nikhilsuvarna/bd0aa0ef01880270c13d145c61a4af22&lt;/a> )&lt;/p>
&lt;h4 id="dynamic">Dynamic&lt;/h4>
&lt;p>As mentioned earlier, Envoy&amp;rsquo;s configuration relies heavily on interface automation to generate various configurations. These configurations can be modified by Runtime without reloading files. In modern application architectures, the life cycle of a service endpoint becomes shorter and its operation Uncertainty or resilience has become greater, so the ability to make runtime changes to the configuration without having to reload the configuration file is particularly valuable in modern application architectures, which is an important consideration for Istio&amp;rsquo;s choice of Envoy as the data plane. Envoy also has a hot restart capability, which makes it more elegant when an upgrade or a restart is necessary, and existing connections can be protected more.&lt;/p>
&lt;p>In the Istio scenario, Envoy&amp;rsquo;s container runs two processes, one called pilot-agent and one is envoy-proxy itself. The pilot-agent is responsible for managing and starting Envoy, and generates an envoy under /etc/istio/proxy/ -rev0.json Initial configuration file, this file defines how Envoy should communicate with the pilot server to obtain the configuration, and use this configuration file to finally start the Envoy process. However, the final configuration of Envoy is not only the content in envoy-rev0.json, it contains all the dynamic configurations discovered through the xDS protocol mentioned above.&lt;/p>
&lt;pre>&lt;code># kubectl exec -it productpage-v1-7f4cc988c6-qxqjs -n istio-bookinfo -c istio-proxy -- sh
$ ps -ef
UID PID PPID C STIME TTY TIME CMD
istio-p+ 1 0 0 Jun25 ? 00:00:33 /usr/local/bin/pilot-agent proxy sidecar --domain istio-bookinfo.svc.cluster.local --serviceCluster productpage.istio-bookinfo --proxyLogLevel=warning --proxyComp
istio-p+ 14 1 0 Jun25 ? 00:05:31 /usr/local/bin/envoy -c etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster productpage.istio-bookin
istio-p+ 142 0 0 15:38 pts/0 00:00:00 sh
istio-p+ 148 142 0 15:38 pts/0 00:00:00 ps -ef
&lt;/code>&lt;/pre>&lt;p>In the envoy overall configuration dump of the following figure, you can see that the contents of bootstrap and other static and dynamic configurations are included:&lt;/p>
&lt;p>&lt;img src="envoy-dump-config-json-struc.jpg.jpg" alt="">
(Envoy configuration structure)&lt;/p>
&lt;p>Combined with the following figure, you can see the basic Envoy configuration structure and its logic, whether it is an entrance listener (similar to F5&amp;rsquo;s VS and part of the profile configuration, NGINX&amp;rsquo;s listener and some Server paragraph configuration) or routing control logic (similar to F5 LTM policy, NGINX&amp;rsquo;s Various Locations matching, etc., or Clusters (similar to F5 pool, NGINX upstream), Endpoints (similar to F5 pool member, NGINX upstream server), and even SSL certificates can be automatically discovered from the service side through the interface&lt;/p>
&lt;p>&lt;img src="envoy-basic-objects-logic.png" alt="">
(picture (From &lt;a href="https://gist.github.com/nikhilsuvarna/bd0aa0ef01880270c13d145c61a4af22">https://gist.github.com/nikhilsuvarna/bd0aa0ef01880270c13d145c61a4af22&lt;/a> )&lt;/p>
&lt;h4 id="scalability">Scalability&lt;/h4>
&lt;p>A lot of filters can be seen in the configuration of Envoy. These are the performance of its scalability. Envoy learned the architecture of F5 and NGINX, and used a lot of plug-ins to make it easier for developers to develop. From the start of listener, it supports the use of filter, and supports developers to develop L3, L4, L7 plug-ins to achieve protocol expansion and more control.&lt;/p>
&lt;p>In practice, companies may not have as many C++ development reserves as languages ​​such as JavaScript, so Envoy also supports Lua and Webassembly extensions. This aspect eliminates the need to frequently recompile binaries and restart, and on the other hand reduces enterprise plug-in development. Difficulty, so that companies can use more Webassembly compatible languages ​​for plug-in writing, and then compile to Webassenmbly machine code to achieve efficient operation. At present, Envoy and Istio are still in the early stages of using Webassembly for expansion, and it will take some time to mature.&lt;/p>
&lt;p>&lt;img src="concept-envoy-filter.png" alt="">
(Picture from &lt;a href="https://www.servicemesher.com/istio-handbook/concepts/envoy.html">https://www.servicemesher.com/istio-handbook/concepts/envoy.html&lt;/a> )&lt;/p>
&lt;p>As can be seen from the above figure, such a request processing structure is very close to the design idea of ​​the F5 TMOS system, and is similar to NGINX to a certain extent. Connections and requests correspond to different processing components at different protocol levels and stages, and these components are themselves extensible and programmable, which in turn enables flexible programming control of the data flow.&lt;/p>
&lt;h4 id="observability">Observability&lt;/h4>
&lt;p>It is said that Envoy is born with the characteristics of cloud native. One of the characteristics is the emphasis on observability. You can see the three observable components: logs, metrics, and tracing are all supported by Envoy by default.&lt;/p>
&lt;p>Envoy allows users to define flexible log formats in flexible locations in a flexible manner. These changes can be delivered through dynamic configuration to achieve immediate effect, and allows the definition of sampling of logs. In Metrics, it provides many indicators that can be integrated with Prometheus. It is worth mentioning that Envoy allows the filter itself to expand these indicators. For example, in the filter such as current limiting or verification, the plug-in itself is allowed to define its own indicators to help users better. Use and quantify the operational status of the plugin. In terms of Tracing, Envoy supports integration with third parties such as zipkin, jaeger, datadog, lightStep, etc. Envoy can produce a uniform request ID and keep it spread throughout the network structure. It also supports external x-client-trace-id to achieve A description of the relationship topology between microservices.&lt;/p>
&lt;p>&lt;img src="envoy-kiali.jpg" alt="">&lt;/p>
&lt;p>Each span generated by Envoy contains the following data:&lt;/p>
&lt;ul>
&lt;li>Set by the &amp;ndash;service-clusteroriginal service cluster information.&lt;/li>
&lt;li>The start time and duration of the request.&lt;/li>
&lt;li>Set by the &amp;ndash;service-nodeoriginal host information.&lt;/li>
&lt;li>By x-envoy-downstream-service-clusterdownstream cluster header set.&lt;/li>
&lt;li>HTTP request URL, method, protocol and user agent.&lt;/li>
&lt;li>By custom_tagsanother custom label settings.&lt;/li>
&lt;li>The upstream cluster name and address.&lt;/li>
&lt;li>HTTP response status code.&lt;/li>
&lt;li>GRPC response status and messages (if available).&lt;/li>
&lt;li>Error flag when HTTP status is 5xx or GRPC status is not &amp;ldquo;OK&amp;rdquo;.&lt;/li>
&lt;li>Track system-specific metadata.&lt;/li>
&lt;/ul>
&lt;h4 id="modernity">Modernity&lt;/h4>
&lt;p>In fact, it is obviously correct nonsense to say that Envoy has modernity. Envoy was born for modern application architecture. Here we mainly want to explain from several aspects that we can most easily feel. The first is its special structural design. In Envoy, it supports the use of iptables to intercept the traffic and do transparent processing. It can use getsockopt () to extract the original destination information in the NAT entry, and allow listeners to listen on the listener. The transferred port listener jumps to an unbound listener that actually matches the original destination information. Although from the perspective of a reverse proxy, this is a bit like F5&amp;rsquo;s VS internal jump, NGINX&amp;rsquo;s subrequest, but its biggest feature and ability lies in transparent connection, which is especially important in the deployment Pod sidecar mode, refer to specific principles herein .&lt;/p>
&lt;p>For the gray-scale publishing, traffic mirroring, circuit breaker, global current limiting and other functions that are favorite for modern applications, its configuration is also very simple. Although F5/NGINX and other software can also accomplish similar tasks, they are native Envoy has greater advantages in terms of ease of configuration and ease of configuration.&lt;/p>
&lt;p>Another manifestation of modernity is the support of the protocol. Look at the following supported protocols. Students who are familiar with application delivery and reverse proxy software may not help but express their admiration. The support of these protocols on the other hand shows Envoy’s A feature that is more oriented towards developers and SRE.&lt;/p>
&lt;ul>
&lt;li>gRPC&lt;/li>
&lt;li>HTTP2&lt;/li>
&lt;li>MongoDB&lt;/li>
&lt;li>DynamoDB&lt;/li>
&lt;li>Redis&lt;/li>
&lt;li>Postgres&lt;/li>
&lt;li>Kafka&lt;/li>
&lt;li>Dubbo&lt;/li>
&lt;li>Thrift&lt;/li>
&lt;li>ZooKeeper&lt;/li>
&lt;li>RockeMQ&lt;/li>
&lt;/ul>
&lt;h3 id="deployment-architecture">Deployment architecture&lt;/h3>
&lt;p>After understanding the technical characteristics of Envoy, let&amp;rsquo;s look at Envoy from the perspective of deployment architecture.&lt;/p>
&lt;p>Complete Sidecar model deployment, which is the biggest deployment feature of Envoy. The communication between services is completely transformed into the communication between Envoy agents, so that many non-business functions are removed from the service code to external proxy components. Envoy is responsible for network communication control Observable with flow. It can also be deployed as a simplified sidecar, which only acts as a proxy for the inbound direction of service without additional traffic manipulation. This structure is used in the external observability based on NGINX to achieve business observability
&lt;img src="t1.jpg" alt="">&lt;/p>
&lt;p>Hub type, which is the same as the Router-mesh type concept in NGINX&amp;rsquo;s MRA. All services use a centralized Envoy to communicate. This deployment structure is generally suitable for small and medium-sized services. Service flow can be directed by adapting to service registration. To Envoy
&lt;img src="t2.jpg" alt="">&lt;/p>
&lt;p>Envoy can also be used as an Ingress edge gateway or Egress gateway. In this scenario, Envoy is generally used for Ingress controller or API gateway. You can see that many such implementations like to use Envoy as the underlying layer, such as Gloo, Ambassador, etc.
&lt;img src="t3.jpg" alt="">&lt;/p>
&lt;p>The following deployment structure should be familiar to everyone. As an Edge gateway, Envoy also deploys an additional layer of microservice gateway (or proxy platform layer)
&lt;img src="t5.jpg" alt="">&lt;/p>
&lt;p>Finally, this is to integrate all forms of Envoy deployment. This architecture may be in the middle of the process of migrating services from traditional architecture to microservice architecture
&lt;img src="t4.jpg" alt="">&lt;/p>
&lt;p>Ok, take a look at how Envoy is used in Istio
&lt;img src="t6.jpg" alt="">&lt;/p>
&lt;p>In summary, due to the cross-platform nature of Envoy, it has the same flexible deployment structure as NGINX, but in fact the deployment structure often has a strong relationship with the final configuration implementation mechanism, can the software&amp;rsquo;s ability adapt to the flexibility under this structure Implementation with simple configuration is the ultimate test. Objectively speaking, Envoy has an advantage in this respect.&lt;/p>
&lt;h3 id="software-architecture">Software Architecture&lt;/h3>
&lt;p>Envoy adopts a single-process multi-thread design structure, and the main thread is responsible for configuration updates and process signal processing. Requests are handled by multiple worker threads. In order to simplify and avoid processing complexity, a connection is always handled by one thread, which can minimize some lock operations caused by data sharing between threads. Envoy avoids state sharing between threads as much as possible, and designed the Thread Local Store mechanism for this purpose. In the log writing, the worker thread actually writes to the memory cache, and finally the file refresh thread is responsible for writing to the disk, which can improve efficiency to a certain extent. Overall, Envoy is still more focused on simplifying complexity and emphasizing flexibility, so unlike NGINX, it does not put the pursuit of performance in the first place, which can be obtained in the relevant official blog of Envoy verification.&lt;/p>
&lt;p>&lt;img src="envoy-thread.png" alt="">&lt;/p>
&lt;p>Similar to NGINX, Envoy is an asynchronous, non-blocking design, using an event-driven approach. Each thread is responsible for each listener, SO_REUSEPORT can also be used to share sockets, NGINX also has a similar mechanism.&lt;/p>
&lt;p>&lt;img src="t7.jpg" alt="">&lt;/p>
&lt;p>After the listener listens and starts processing, the connection will be processed by subsequent L3, 4, 7 and other filters according to the configuration.&lt;/p>
&lt;p>&lt;img src="envoy-arch.jpg" alt="">&lt;/p>
&lt;h2 id="f5nginx-the-sword-is-not-out">F5/NGINX: the sword is not out&lt;/h2>
&lt;p>After understanding the technical characteristics and architecture of Envoy, we return to the original point of this article. Envoy has been carrying the genes of modern application architecture from birth, does it mean that these front waves such as NGINX/F5 are out of date.&lt;/p>
&lt;p>I remember the author of NGINX, Igor, at the F5 China 520 conference to explain why NGINX is so successful. He said that he did not expect to be so successful because the reason is that he developed the right software at the right time. We know that during the period around 2003, there was still no talk about distributed architecture and microservices. At that time, the main problem to be solved was stand-alone performance. Based on this background, NGINX is strict in terms of architecture design and code quality. Demanding performance. In terms of functionality, NGINX was originally a Web Server software, L7 reverse proxy is an extension of its capabilities, and L4 proxy capabilities increase even later. In view of this background, from the perspective of modern application architecture, there are indeed some Capability is more difficult to cover. Similarly, Envoy was born and developed in the era of modern application architecture. As Envoy self-explained, it refers to a large number of existing hardware and software reverse proxy and load balancing products. From the above technical analysis, it can also be seen that Envoy has many NGINX and F5 Architectural concept, it can be said that Envoy draws many essences from mature reverse proxy products, and fully considers the needs of modern application architecture when designing, it is also a correct software at the right time.&lt;/p>
&lt;p>Under the microservices architecture, many problems have become how to control the communication and traffic insights between services. This is a typical application delivery field. As a frontier in this field, on the one hand, we must actively embrace and adapt to the new era of application architecture. On the one hand Need to innovate and continue to lead new directions. There have been two technological innovations in this field in history. The first was around 2006, when the topic of &amp;ldquo;load balancing was dead&amp;rdquo; was fired. The essence was that the market began to change at that time, and everyone was no longer satisfied with simple loads. Balanced, demand is derived from more complex scenarios such as application security, network optimization, application optimization, access control, and flow control. The concept of application delivery began to be proposed. It can be said that before 2006, the main concepts and technical directions of the market were based on The four-layer switch is the core concept of load balancing technology. Most players are traditional network manufacturers. The thinking and concepts are based on network switching. F5 is like a strange guy. The product design thinking is completely on another dimension. The TMOS V9 operating system, which has been released since 2004, has led the market since then, and no one has surpassed it for 10 years thereafter. The second technological innovation occurred around 2016. Affected by the cloud and microservices, software and lightweight became the mainstream of the market. At the same time, Devops thought means that the role of users has changed. The traditional design for network operation and maintenance personnel It began to become difficult to meet market demand. The field dominated by F5 has also undergone new changes in the market. Gartner no longer publishes magic quadrant analysis in the field of application delivery, and instead forms guidance in the way of Guide.&lt;/p>
&lt;p>&lt;img src="F5-stock.jpeg" alt="">&lt;/p>
&lt;p>Looking at the present, history is always surprisingly similar.&lt;/p>
&lt;p>The modern application architecture is developing rapidly, and a large number of applications are beginning to be micro-serviced. However, from the perspective of the overall chain of business access, Envoy cannot solve all problems, such as application security protection, complex enterprise protocols, and different needs caused by different organizational relationships. It can be seen that the application delivery products represented by F5/NGINX have also begun to actively realize product integration under the Devops tide. F5 has released a complete automated tool chain, from the product’s bootstrap to network configuration, to application service configuration, to the final Monitoring and telemetry have formed a complete interface, and use declarative interface to promote product management to a higher role crowd and management system. NGINX also builds its own API and Controller plane, and provides a declarative API interface to the outside world. Developers can better use the interface to integrate into their own control plane. These changes are for developers or SRE to better use F5/NGINX. For details, please refer to my &amp;ldquo;From Traditional ADC to Cloud Native ADC&amp;rdquo; &lt;a href="https://mp.weixin.qq.com/s?src=11&amp;amp;timestamp=1593224168&amp;amp;ver=2425&amp;amp;signature=znUdlLDdpbGGxWX7pZhH2uSVq1SAdQuloO09HIXssdQ15nRtWVOIgzlYTFmjOIUsDrqghPbSZM6vQI45TIqmINQKjposI7AfJ6jKQaEXm9KD4tEV5Bk9AF0RGuKvVuHI&amp;amp;new=1">series of articles&lt;/a>.&lt;/p>
&lt;p>&lt;img src="slides-3.jpg" alt="">&lt;/p>
&lt;p>After acquiring NGINX and Shape, F5 put forward a new view that will make full use of the widely accessible data plane capabilities, and use AI to further tap the data potential to help users better grasp and understand application behavior and performance, and provide references for business operations. , And feedback to component configuration and operation management to form a closed loop.&lt;/p>
&lt;p>An important scenario for modern application delivery is still indispensable, that is, application security. Although Istio and other products have made many attempts in secure communication, identity, and strategy, application security itself is relatively lacking. F5 is a leading manufacturer in the field of WAF security Through the transfer of security capabilities to NGINX, a new NGINX APP Protect is formed, which uses its cross-platform capabilities to help users better manage application security capabilities in microservice scenarios and help enterprises better implement DevSecOps.&lt;/p>
&lt;p>If we compare the technical features of Envoy with F5, we can see that F5 lacks scalability and modernity to a certain extent. F5 has good programming control capabilities, but it is relatively larger than the development of larger plug-ins. Insufficient, this and modernity can often be linked together. For example, if you want to make a complex 7-layer filter similar to Envoy for a very new protocol, it is impossible to achieve, although iRule or iRuleLX can do something to a certain extent. However, in any case, the final product form of F5 itself determines that F5&amp;rsquo;s BIGIP cannot be completely cross-platform, because it cannot run as a container. It is worth expecting that such morphological restrictions will be broken by F5&amp;rsquo;s next-generation TMOS system.&lt;/p>
&lt;p>Service Mesh is the current popular technology direction. F5 builds an enterprise-level Aspen Mesh service mesh product based on Istio, which helps enterprises deploy and use Istio better and easier. Aspen mesh team members enter the Istio Technical Oversight Committee with only 7 positions and are responsible for the important responsibilities of Istio&amp;rsquo;s RFCs/Designs/APIs. Although Istio has absolute ecology and popularity in the field of service mesh, this does not mean that Istio is the only choice. In many cases, customers may want to adopt a more concise Service Mesh to achieve most of the required functions instead of deploying one. The entire complex Istio solution, NGINX Service Mesh (NSM) based on NGINX components will bring new choices to users, a more simple and easy to use Service Mesh product, this is the reason why we mentioned NGINX to terminate Nginmesh at the beginning of the article .&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>Technology development is an inevitable process. In 2006, it evolved from traditional load balancing technology to application delivery. In addition to load balancing, it introduced many aspects such as security, access control, access control, and flow control. Around 2016, new technological changes have occurred in this field again. The emergence of a large number of new generation reverse proxy open source software has a new impact on traditional application delivery products. Active adaptation and change and innovation are the key to winning. Envoy has excellent capabilities as a new representative, but it is not a silver bullet to solve all problems. Envoy has a steeper learning curve and higher development and maintenance costs. For enterprises, they should choose the appropriate solution and Products to solve different problems in the architecture, to avoid catching the trend and let yourself fall into the trap.&lt;/p>
&lt;p>F5 needs more to let developers understand the huge potential of TMOS system (especially the subversion of the next generation products in architecture and form), understand its excellent all-agent architecture and program control at any level, so that developers, SRE develops with F5 TMOS as a capability platform and middleware, and better utilizes F5&amp;rsquo;s own application delivery capabilities to quickly realize its own needs.&lt;/p>
&lt;p>Finally, again quote a sentence from the homepage of the official Envoy website:&lt;/p>
&lt;blockquote>
&lt;p>As microservice practitioners soon realized, most of the operational problems that arise when moving to a distributed architecture are ultimately based on two aspects: network and observability.&lt;/p>
&lt;/blockquote>
&lt;p>And to ensure more reliable network delivery and better observability is the strength of Qianlang. Innovate, Qianlang.&lt;/p>
&lt;p>Written at the end: No matter how the technology changes, the human factor is still the core, regardless of the company or the manufacturer, in such a wave of technology, it should have sufficient technical reserves, just like the traditional financial industry through the establishment of technology companies to seek transformation, Manufacturers also need to be transformed. F5 China&amp;rsquo;s SE has almost 100% passed the CKA certification. Regardless of the relative proportion or absolute number, it should be unique in the industry. The transformation is not only in products, but also in thinking.&lt;/p>
&lt;p>Check more istio practice detail at my tech blog &lt;a href="https://imesh.club">https://imesh.club&lt;/a>&lt;/p></description></item><item><title>What changes does envoy bring to ADN</title><link>http://linjing.io/publication/envoy-2020-6/</link><pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate><guid>http://linjing.io/publication/envoy-2020-6/</guid><description>&lt;!-- raw HTML omitted -->
&lt;p>Check here for full article &lt;a href="https://www.servicemesher.com/blog/thoughts-to-envoy-from-adn-perspective/">The link&lt;/a>.&lt;/p></description></item><item><title>NGINX and oAuth2/OIDC series one</title><link>http://linjing.io/post/nginx-oauth2-oidc-series/</link><pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate><guid>http://linjing.io/post/nginx-oauth2-oidc-series/</guid><description>&lt;p>Nowadays, the Internet has penetrated various life and business scenarios. Just like people have never been a simple individual in real life, we need to have many complicated relationship networks. The same is true of Internet applications, and it is now difficult to see which application can develop completely independently without having a relationship with its surroundings. Therefore, today&amp;rsquo;s applications are very particular about their own ecology, and they need to share a lot of information with each other. These complex relationships raise a very important issue: identity authentication, resource authorization, and account maintenance. Of course there is API authentication access control.&lt;/p>
&lt;p>For example, in your daily life, you may need to use dozens of apps. Each of these apps has an independent account and password. You need to maintain different accounts and passwords. You may have one set of accounts for all apps in order to save trouble. Password, so after the application A was violently vaulted, you are forced to change all the passwords of the accounts of these dozens of apps. You may also be more concerned about the security of your account, so you use a fixed + changing password to combine dozens of APP accounts, which is very good, can help alleviate a large part of security issues, and at the same time reduce your password maintenance. Question (Is the memory okay), but these apps may not be able to follow the fixed + change mode you envisioned as you wish, some may only support numbers, some support numbers plus passwords, and some still It requires a minimum length and a more complicated combination, so you start to use a small book to record the password format of different applications (well, at least there was a stage where I did this, recorded in a description language on the computer, when forgotten At that time, go to find this hint in the computer).&lt;/p>
&lt;p>Is there a better way?&lt;/p>
&lt;p>If you trust a company that has done well in security and has a good reputation system, can you use this account to run the Internet? I believe we already have the answer. Today, we may have used it a lot of times. When you log in to the XX application, you are used to skip user registration and go to click &amp;ldquo;log in with ***&amp;rdquo;, in the pop-up interface Here is a very sacred point under &amp;ldquo;Agree&amp;rdquo;. So you no longer need to remember so many application accounts. This is actually a typical open authorization (Open Authorization) referred to as oAuth (current version is 2, also known as oAuth2).&lt;/p>
&lt;p>Um, you seem to be lying to me. You have said so much, it seems to be all about authentication, why it is said to be Authorization. Yes, you are right, but I did not lie to you, but there are some silly problems that are not easy to distinguish. The original purpose of this oAuth design is to solve the problem of data access between interest groups, just like us As mentioned at the beginning, there is a problem of mutual access to a large amount of data between applications of different companies. For example, company A has developed an online photo printing application, but this company does not operate photo storage services. Your photos may exist in B, C, D. On the network disks of different companies (Yes, in order to take advantage of the early days, I did occupy a lot of the network disks of many companies. Later, some of their network disks used rogues to send a notice and couldn&amp;rsquo;t do anything. Fortunately, I used RAID1), so this caused problems. How do you send photos to this online printing company, download them from the BCD network disk and send them to them? Give the account number and password of the network disk to the printing company? Obviously these methods will not work. If you rely on downloading and uploading, it is estimated that you are too lazy to get it. If you are giving an account password, unless you are not sober.&lt;/p>
&lt;p>For printing companies and network disk service providers, they also have similar troubles. If users are allowed to upload and download, the user experience is too bad, and they also maintain a whole set of such systems. Therefore, printing companies hope that there is a simple way to connect at the same time. BCD network disk company, as long as one of the users of these network disks agrees, it will automatically pull down the user photos from these network disks to print, and own 0 inventory. For the BCD network disk company, storing cold data alone is obviously not the purpose. Moreover, you are still in piao, you have to do tricks, so the network disk company also wants to dock these companies that print pictures, but for them It is necessary to solve the user&amp;rsquo;s security issues on accounts and photos.&lt;/p>
&lt;p>So it can be seen that for these three different stakeholders, there is a desire to have something to solve their problems at the same time. This is authorization. When the user wants to print the photo, the printing company guides the customer to enter the network disk interface. The user is Log in to the network disk and authorize the network disk to allow which of my resources to be shared with the photo printing company. For example, share your beautiful photos to print, and the original photos are not allowed to be accessed by the printing company, which is very safe. So we can summarize:&lt;/p>
&lt;p>&lt;img src="https://imesh.club/upload/2020/05/1589076716809-1024x441.jpg?v=1589076733" alt="">&lt;/p>
&lt;p>oAuth is used to solve such a scenario, so you can see that it is an authorization process. But you haven’t said why it was a certification at the beginning, hmmm. After all, I also spent a lot of time to learn it. It is also a process after finishing it. Just like this article, it is a series. Only the following articles can be finished:&lt;/p>
&lt;ol>
&lt;li>NGINX as (Client) role and resource service role proxy in oAuth, Authorization code mode (with oAuth proxy service)-this article&lt;/li>
&lt;li>NGINX as (Client) role and resource service role proxy in oAuth, Authorization code mode (Without the help of oAuth proxy service)&lt;/li>
&lt;li>NGINX authenticates and recognizes id_token in OIDC (Implicit mode)&lt;/li>
&lt;li>NGINX acts as a resource service role in oAuth, proxy authentication and identity information recognition (Token introspection)&lt;/li>
&lt;/ol>
&lt;p>Why is authentication involved here? In fact, you will find that during the authorization process, the identity is obviously inescapable. The authorization must be based on a certain user, so the oAuth specification does not emphasize that you can’t do this. In addition to authorization, plus sometimes, we really do not need authorization scenarios, but want to reduce our account maintenance, use one company account to log in many products of other companies, so there are a lot of oAuth For authentication scenarios, of course, it is precisely because oAuth does not make many standardized definitions for authentication, which leads to different designs of programs of different companies when implementing authentication. There is no standard way to obtain user information. A common standard scope, based on this, OpenID Connect (OIDC) appeared. OIDC is based on oAuth. The communication process of several parties is the same. The difference is that OIDC is sending to IdP (the party that stores the account and performs verification). When the request is initiated, the openid tag will be brought in the scope. In the end, the information returned by the IdP will also carry an ID token (JWT) in addition to the oAuth normal access_token, and the application can use it as a login after getting this ID token. If you need more additional information, you can take the access_token and go to the userinfo endpoint to get more user information. In addition, OIDC is a protocol family and contains many other specifications, such as session management and registration discovery. Because oAuth and OIDC are very similar in communication mechanism, we often confuse the two. We often say that oAuth authentication should actually be oAuthZ, and OIDC is oAuthN.&lt;/p>
&lt;p>Back to this article, in this article, we will follow the interaction of a standard oAuth authorization code mode to see what NGINX can help users do here, and why NGINX is needed to do such a thing.&lt;/p>
&lt;p>First, we need to sort out the entire interaction process in the oAuth authorization code mode. In order to avoid the obscurity of RFC , let&amp;rsquo;s assume a scenario.&lt;/p>
&lt;p>You are in a startup company, such as a company engaged in AI and big data (of course, it is not listed yet, it is listed, and you may not have time to read this article), your company uses a lot of cloud service examples, buy servers to engage in computer rooms , Engage in infrastructure, that is not a thing. You have used open source to build a lot of systems, and quickly put your business online. Everyone knows that open source systems have a great feature, which is friendly to developers. What does it mean, how is it convenient (that is, developers are lazy, non- To be straight&amp;hellip;), so you see that many open source systems don’t think about authentication, and you visit it after installing it. It seems that there is no account authentication as a matter of course. At first, it didn’t matter, because you were alone, what You have to do everything, as more and more systems, employees begin to increase, you need to make some restrictions on access to different systems for different people, and you are still going to go public, as a public company, your system There is no account, so it is unreasonable. Then you have some application development systems that need to connect to the github API. You need to allow only some advanced developers to access a private repo. And you have no time to build a new user management system yourself. Fortunately, these people have github accounts, so you can use these github accounts to do the simplest and fastest things. These requirements can be summarized as:&lt;/p>
&lt;p>A function needs to be implemented on different systems to enable these systems to interface with github, and use the github account to determine whether employees can access a certain system
Log in with the github account on the application development system, and apply for resource authorization from github to include the person’s repo and other information. If the person does not have the private repo permission, the natural application development system cannot obtain the private under the permission of this employee. repo content
These requirements oAuth can help solve, but there is a problem. If you join the oAuth mechanism, you need to develop on the system. So many open source systems, the development language is different, and even some systems dare not rush to redevelop. It is actually very difficult to achieve and the workload is actually very large.&lt;/p>
&lt;p>Before looking at what NGINX can do, let&amp;rsquo;s take a look at the oAuth process without NGINX and the above requirements.&lt;/p>
&lt;p>&lt;img src="https://imesh.club/upload/2020/05/1589087602055-1024x534.jpg?v=1589087614" alt="">&lt;/p>
&lt;p>From the above process, it can be seen that for users, they log in and authorize once on github, and the browser makes two jumps. The really useful access_token is between the back-end application server and github. The user and the browser itself cannot see the content of this access_token, which is called the backend channel and is relatively safe. So what does the application do after getting this access_token?&lt;/p>
&lt;p>-If it is limited to obtaining some basic information of the user, and the returned access_token is JWT, then the application server can obtain the content in the JWT by itself, so that the user information extraction is associated with some local user IDs, which can be used as Used for login (of course, if it is pure identity authentication and this joint login scenario, in fact, OIDC should still be considered). Of course, if the access_token here is opaque, then the application server also needs to do token introspection, that is, it needs to be verified again with the authorized party before the relevant information can be used.&lt;/p>
&lt;p>-If it is not limited to obtaining user information, but to obtain additional resources, such as the need to obtain the person&amp;rsquo;s repo content, then the application server needs to access this access_token to access a github repo resource server (resource server and The authorization servers are not necessarily the same, and large-scale scenes are usually not the same) to obtain the person&amp;rsquo;s repo content, then the above picture becomes like this:&lt;/p>
&lt;p>&lt;img src="https://imesh.club/upload/2020/05/1589088979883-1024x570.jpg?v=1589089012" alt="">&lt;/p>
&lt;p>So, you will find that the web application backend is very critical. It participates in the entire oAuth process and finally obtains the access_token. Imagine, as you said at the beginning of the company, many open source are developed in different languages. System, you have to transform to add this ability. At this time, you actually only want to decide based on the user&amp;rsquo;s information that the system must be logged in through the oAuth process before it can be accessed, or the system determines who can access based on the user name.&lt;/p>
&lt;p>This work can actually be achieved by placing NGINX in front of the web application backend, which means that NGINX is allowed to participate in the oAuth authentication process on behalf of the backend application, and then NGINX can decide whether to allow or reject certain users based on access_token, or Transparently transmit user information to back-end applications for more processing.&lt;/p>
&lt;p>Carefully observe the entire verification process above, which requires NGINX to participate in the construction of the jump return, and use the authorization code to construct the request to directly access the github authorization server. If these tasks are done purely on NGINX, it is actually very difficult. Development through njs is a way but requires the ability to authenticate JWT (so NGINX Plus does not need to install an oauth proxy service like the demo in this article, It can be realized by directly using the njs module + KV module + JWT module. For details, please refer to the second part of this series), but in fact, it can be achieved with the help of the ability of auth_request and an oAuth proxy, which means that we need to be in various The implementation code of the oAuth authentication process created on the open source system is abstracted to it, and a general one is involved. The oAuth proxy agent participates in this oAuth process, and finally the obtained access_token is parsed out. The relevant claims information is returned to NGINX, NGINX Based on this information, we will control whether to allow access to a resource, or transparently pass relevant user information to the final application. So its implementation logic is as follows:&lt;/p>
&lt;p>&lt;img src="https://imesh.club/upload/2020/05/1589090989794-1024x566.jpg?v=1589091004" alt="">&lt;/p>
&lt;p>The idea and principle of implementation (the following serial number has nothing to do with the figure):&lt;/p>
&lt;ol>
&lt;li>Configure NGINX to publish protected applications&lt;/li>
&lt;li>Configure auth_request under the location section of the relevant application&lt;/li>
&lt;li>In this way, when the request reaches NGINX, NGINX will initiate the sub-request authentication by auth_request&lt;/li>
&lt;li>The sub-request will be proxy_pass to an interface of the oauth proxy service&lt;/li>
&lt;li>According to the characteristics of auth_request, it is necessary for oauth proxy to return the relevant status code to indicate whether NGINX is released or returns 401&lt;/li>
&lt;li>Therefore, after receiving the sub-request, the oauth proxy will determine whether the user has previously completed the relevant oauth authentication work. If the user has not logged in, or the validity period has expired, then the oauth proxy returns 401 (here depends on whether the user browser carries the oauth proxy Issued by a cookie information to check)&lt;/li>
&lt;li>NGINX intercepts the status of 401, and implements the definition of error_page to send a 302 jump to the user&amp;rsquo;s browser if 401 is returned. The address of this jump is actually a special interface of oauth proxy used to trigger the subsequent oAuth process. The subsequent process is no different from normal oAuth.&lt;/li>
&lt;li>After the oAuth proxy completes the entire oAuth process, it returns a 302 jump to the user browser, and this return will also carry the relevant cookie to allow it to revisit the protected application&lt;/li>
&lt;li>After NGINX receives the request, it triggers auth_request again. auth_request sends the request to an interface of oauth proxy again. This visit carries the cookie in 8. This way, the oauth proxy knows who it is based on the cookie, and resolves its access_token to pass relevant claims. Put it in the response header and return to NGINX&lt;/li>
&lt;li>Use auth_request_set to put the claims in the response header of the sub-request into variables and pass it to the parent request&lt;/li>
&lt;li>NGINX judges whether to release based on these variables, or puts these user information in the request header to pass the content to the last protected application&lt;/li>
&lt;/ol>
&lt;p>There are many implementations of such an oauth proxy online, here is a brief list:
&lt;a href="https://github.com/vouch/vouch-proxy">vouch-proxy&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://github.com/oauth2-proxy/oauth2-proxy">oauth2 proxy&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://github.com/jirutka/ngx-oauth">oauth2 proxy by lua -implement proxy directly in lua, no need to install additional proxy service&lt;/a>&lt;/p>
&lt;h2 id="demo">Demo&lt;/h2>
&lt;p>This demonstration uses NGINX plus and vouch-proxy to achieve. For the specific installation and configuration of vouch-proxy, please refer to its github directly, it is not complicated&lt;/p>
&lt;p>&lt;img src="https://imesh.club/upload/2020/05/1589093713142-1024x705.jpg?v=1589093738" alt="">&lt;/p>
&lt;p>In the actual demo, the web application backend actually has an intermediate NGINX to simulate, using return to return the content.&lt;/p>
&lt;p>NGINX configuration:&lt;/p>
&lt;pre>&lt;code>############entry for protected app http://authcode.imesh.club/personalinfo
server {
listen 80;
server_name authcode.imesh.club;
#root /var/www/html/;
# send all requests to the `/validate` endpoint for authorization
auth_request /validate;
#The location is for auth_request subrequest
location = /validate {
# forward the /validate request to Vouch Proxy
proxy_pass http://127.0.0.1:9090/validate;
# be sure to pass the original host header
proxy_set_header Host $http_host;
# Vouch Proxy only acts on the request headers
proxy_pass_request_body off;
proxy_set_header Content-Length &amp;quot;&amp;quot;;
# optionally add X-Vouch-User as returned by Vouch Proxy along with the request
auth_request_set $auth_resp_x_vouch_user $upstream_http_x_vouch_user;
# these return values are used by the @error401 call
auth_request_set $auth_resp_jwt $upstream_http_x_vouch_jwt;
auth_request_set $auth_resp_err $upstream_http_x_vouch_err;
auth_request_set $auth_resp_failcount $upstream_http_x_vouch_failcount;
}
# if validate returns `401 not authorized` then forward the request to the error401block
error_page 401 = @error401;
location @error401 {
# redirect to Vouch Proxy for login
return 302 http://vouch.imesh.club/login?url=$scheme://$http_host$request_uri&amp;amp;amp;vouch-failcount=$auth_resp_failcount&amp;amp;amp;X-Vouch-Token=$auth_resp_jwt&amp;amp;amp;error=$auth_resp_err;
# you usually *want* to redirect to Vouch running behind the same Nginx config proteced by https
# but to get started you can just forward the end user to the port that vouch is running on
}
# for the real service that being protected
location / {
# forward authorized requests to your service protectedapp.yourdomain.com
##he backend real server also simiulated by this nginx
proxy_pass http://127.0.0.1:8080;
# you may need to set these variables in this block as per https://github.com/vouch/vouch-proxy/issues/26#issuecomment-425215810
auth_request_set $auth_resp_x_vouch_user $upstream_http_x_vouch_user;
auth_request_set $auth_resp_x_vouch_idp_claims_avatar $upstream_http_x_vouch_idp_claims_avatar_url;
auth_request_set $auth_resp_x_vouch_idp_claims_company $upstream_http_x_vouch_idp_claims_company;
auth_request_set $auth_resp_x_vouch_idp_claims_blog $upstream_http_x_vouch_idp_claims_blog;
# set user header (usually an email)
proxy_set_header X-Vouch-User $auth_resp_x_vouch_user;
# optionally pass any custom claims you are tracking
proxy_set_header X-Vouch-IdP-Claims-company $auth_resp_x_vouch_idp_claims_company;
proxy_set_header X-Vouch-IdP-Claims-avatar $auth_resp_x_vouch_idp_claims_avatar;
proxy_set_header X-Vouch-IdP-Claims-blog $auth_resp_x_vouch_idp_claims_blog;
}
}
&lt;/code>&lt;/pre>&lt;p>Simulation configuration of back-end applications&lt;/p>
&lt;pre>&lt;code> server {
listen 8080;
location /personalinfo {
default_type text/html;
set $user $http_x_vouch_user;
set $avatar $http_x_vouch_idp_claims_avatar;
set $company $http_x_vouch_idp_claims_company;
set $blog $http_x_vouch_idp_claims_blog;
return 200 '&amp;amp;lt;html&amp;gt;&amp;amp;lt;head&amp;gt;&amp;amp;lt;meta http-equiv=&amp;quot;Content-Type&amp;quot; content=&amp;quot;text/html; charset=utf-8&amp;quot; /&amp;gt;&amp;amp;lt;/head&amp;gt;&amp;amp;lt;h2&amp;gt;Your personal info:&amp;amp;lt;/h2&amp;gt;&amp;amp;lt;hr /&amp;gt;Name: $user &amp;amp;lt;br&amp;gt;avatar: $avatar &amp;amp;lt;br&amp;gt;company: $company &amp;amp;lt;br&amp;gt;blog:$blog &amp;amp;lt;/html&amp;gt;';
}
}
&lt;/code>&lt;/pre>&lt;p>Responsible for receiving the request configuration initiated by the client browser to the oauth proxy:&lt;/p>
&lt;pre>&lt;code>#######work for vouch login/auth
server {
listen 80;
server_name vouch.imesh.club;
location / {
proxy_pass http://127.0.0.1:9090;
# be sure to pass the original host header
proxy_set_header Host vouch.imesh.club;
}
}
&lt;/code>&lt;/pre>&lt;p>The effect of the visit process:
&lt;img src="https://imesh.club/upload/2020/05/%E5%9B%BE%E7%89%87-1-1-653x1024.png?v=1589095217" alt="">&lt;/p>
&lt;p>&lt;img src="https://imesh.club/upload/2020/05/%E5%9B%BE%E7%89%87-2-682x1024.png?v=1589095226" alt="">&lt;/p>
&lt;p>&lt;img src="https://imesh.club/upload/2020/05/1589096005904.jpg?v=1589096014" alt="">&lt;/p>
&lt;p>The first visit to &lt;a href="http://authcode.imesh.club/personalinfo,">http://authcode.imesh.club/personalinfo,&lt;/a> the browser is automatically jumped to the vouch.imesh.club/login? interface, this jump is actually driven by NGINX&lt;/p>
&lt;p>After receiving it, vouch.imesh.club processes it and asks the browser to jump to the github.com/authorize interface. Since it has not logged in on github, github jumps to the /login interface to let the user log in.&lt;/p>
&lt;p>The login interface appears. After logging in, the authorization will be displayed. Clicking on the authorization will be redirected to vouch.imesh.club (the service address of oauth proxy), which actually returns the authorization code to the oauth poxy service.&lt;/p>
&lt;p>After clicking the authorization, the browser will continue to jump. The github implementation will have the following jump prompt, which is actually the browser to jump to the callback interface of vouch.imesh.club:&lt;/p>
&lt;p>&lt;img src="https://imesh.club/upload/2020/05/%E5%9B%BE%E7%89%87-3.png?v=1589095466" alt="">&lt;/p>
&lt;p>After the callback interface of vouch.imesh.club is accessed, it will drive vouch to initiate access_token acquisition on the server side. At this time, the browser cannot capture it. When vouch has been obtained on the server, it returns a 302 to the browser again. This 302 requires the browser to officially access the application address, and is accompanied by the relevant cookie to the client browser:&lt;/p>
&lt;p>&lt;img src="https://imesh.club/upload/2020/05/1589096907088.jpg?v=1589096914" alt="">&lt;/p>
&lt;p>Finally completed the visit:&lt;/p>
&lt;p>&lt;img src="https://imesh.club/upload/2020/05/1589097012011.jpg?v=1589097024" alt="">&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Use NGINX&amp;rsquo;s auth_request function, and through clever configuration to use oauth proxy to achieve the complete authentication process of oAuth, and pass relevant user information to NGINX to achieve access control and information processing. Except for the back-end, all applications need to develop code to implement oauth verification, so that enterprises can quickly use third-party accounts to control user access&lt;/p>
&lt;h2 id="follow-up">Follow up&lt;/h2>
&lt;p>In this practice, the authorization code mode of oAuth is adopted, and the external oauth proxy service is used. If you do not want to rely on external services and want to implement on pure NGINX, you can refer to the second part of this series .&lt;/p>
&lt;p>Check more oAuth posts of the series at my tech blog &lt;a href="https://imesh.club/?s=oauth">https://imesh.club/?s=oauth&lt;/a>&lt;/p></description></item><item><title>NGINX from zero to hero</title><link>http://linjing.io/talk/f5-nginx-traning-2020/</link><pubDate>Fri, 28 Feb 2020 16:13:30 +0000</pubDate><guid>http://linjing.io/talk/f5-nginx-traning-2020/</guid><description>&lt;!-- raw HTML omitted --></description></item></channel></rss>