<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NGINX on Jing Lin‘s profile</title><link>http://linjing.io/tags/nginx/</link><description>Recent content in NGINX on Jing Lin‘s profile</description><generator>Source Themes academia (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright &amp;copy; {year} linjing.io host on github, imesh.cloud host on netlify. CI/CD by github actions and netlify.Thanks bootcdn support for front libary CDN</copyright><lastBuildDate>Mon, 27 Jun 2022 00:00:00 +0000</lastBuildDate><atom:link href="http://linjing.io/tags/nginx/index.xml" rel="self" type="application/rss+xml"/><item><title>Analysis and Thinking of Enterprise Open Source</title><link>http://linjing.io/post/analysis-and-thinking-of-enterprise-opensource/</link><pubDate>Mon, 27 Jun 2022 00:00:00 +0000</pubDate><guid>http://linjing.io/post/analysis-and-thinking-of-enterprise-opensource/</guid><description>&lt;blockquote>
&lt;p>近年来，开源软件对企业数字化转型的促进作用愈发明显。国家更是把建设和完善开源生态写入了“十四五规划”。与此同时，围绕开源治理、安全供应链等话题成为业界一大热点。作为开源用户或者作为开源主体，我们应该如何理解开源，如何理解社区，如何更好更安全的参与开源。本文将与您一起从开源的意义、商业模式、风险与挑战、选择与治理等几个方面进行探讨。&lt;/p>
&lt;/blockquote>
&lt;h3 id="开源对企业的意义">开源对企业的意义&lt;/h3>
&lt;p>让我们首先站在一个小我的角度思考开源与个体的意义，可以用一句话来概括“利己与利他”。使用开源项目或产品可以帮助个人快速解决工作中的问题与挑战，个人还可从开源项目或产品中获得创新思想与经验。这些都是利己。当个体不满足于只是作为开源的使用者，而开始参与到开源项目的贡献时，乃至自己设立开源项目时，就变成了利他，其他用户将从他的贡献或项目中获得收益。&lt;/p>
&lt;p>对企业来说也一样。当企业注重自身的数字化转型时，就意味着企业将会追求更加开放的组织精神。数字化转型的核心是利用数字技术和能力实现企业流程再造，进而驱动商业模式创新、提升企业竞争力，甚至改变一个行业，就像“滴滴“一样。这就要求企业的IT基础设施、技术不但要能支撑业务的快速变化与发展，更要能够驱动业务创新。企业要快速建立业务，快速迭代业务，快速获取业务反馈。因而具有可灵活支持短期与长期业务发展的IT架构与技术是关键，企业必须采用开放的可协作的新技术来持续改进和提升技术架构迭代能力。这些都意味着要足够“快”，要足够“创新”。开源技术恰好呼应了企业这样的需求，因为开源技术追求开放、协作、贡献的精神。企业采用开源技术，可以获得来自全世界优秀人才的贡献，更可以通过借助全世界人才的贡献来快速改进和提升企业的竞争实力。开源所造就的开放生态系统为所有相关方带来了这样的价值。&lt;/p>
&lt;p>以智能汽车行业为例。传统上，汽车生产企业需要依赖大量第三方供应商与技术。但当企业进入智能汽车新赛道后，车企需不断的创新车辆功能，提升用户体验，快速占领市场。如果车企依然采用传统供应商模式，将受制于那些封闭的供应商而无法创新产品，无法快速占领市场。通过采用开源技术，比如AGL（Automotive Grade Linux），可以无需再去开发车载的基本操作系统，而将宝贵的研发技术投入在更多的具有竞争性的创新上。&lt;/p>
&lt;p>具有开放性精神的企业，并不会仅仅止步于采用开源技术，更会贡献到开源中来。大型企业可以通过开源自身的产品或技术，帮助企业实现市场战略，占领市场或改变市场游戏规则，例如Google的安卓系统。而创新型小企业通过开源可以积累庞大的社区用户，增加影响力，扩大自身估值。&lt;/p>
&lt;p>可以看出开源对企业数字化转型的意义是显而易见的。一方面，开源有利于激发企业技术创新，相对于使用闭源软件，开源技术让使用者更具创造力和创新精神；另一方面，使用开源技术能够帮助企业节约成本，从而可以让更多的IT投资用于部署新技术，加快数字化转型。在一项针对全球IT领导者2020企业开源状况的调查中同样反映了这一结果，95%的受访者表示开源对他们的整体软件战略具有战略重要性。在国内，一些头部互联网企业也成立了专门的企业开源委员会来帮助企业实现更好的开源战略。国家更是把建设和完善开源生态写入了“十四五规划”。&lt;/p>
&lt;p>开源有时还会被赋予“可信、可控”的概念。诚然，开源并不能和“可信、可控”直接画上等号，只有当企业有能力并有资源来通过学习彻底吃透并掌握时才具备真正的可信、可控。但开源，确实通过源代码开放的形式，让用户可以吃上定心丸，并在企业必要的时候通过研究开源代码来解决运维中的问题。&lt;/p>
&lt;p>F5在中国运营开源NGINX，深刻理解中国用户的诉求。NGINX技术的可靠性与易开发性保证了企业能够具有更好更可靠的IT基础架构，这是它对企业数字化转型的重要意义。无论是在分布式还是在微服务架构里， NGINX技术都已经被全世界用户验证，其可靠性确保了IT基础架构的稳定。在中国，我们可以看到无论是互联网公司，还是金融企业，都有大量的用户在生产中使用NGINX技术，比如农业银行的开放银行，新网银行的业务访问调度等等。NGINX的易开发性让这些技术创新成为可能。可以说很多客户借助NGINX实现了软负载的自主可控。更进一步，通过对开源NGINX进行本地化，在中国提供开源订阅服务，意味着中国用户可以通过本地服务实现更可靠的产品使用，这个服务包含了基本的技术支持，还包含高级专家服务，联合共创、开发等。&lt;/p>
&lt;h3 id="开源的商业模式">开源的商业模式&lt;/h3>
&lt;p>开源的商业模式有很多。根据项目的起源、背景、诉求、主体的不同，一般可以分为：
广告模式，通过在网站、安装过程、文档等地方放入赞助商广告获得一定收益。游戏、搜索类项目，一般容易采取此类方式。&lt;/p>
&lt;p>&lt;strong>售卖服务&lt;/strong>，这是比较典型的开源商业模式。服务商本身并不提供代码的售卖，而是出售服务，用户为更好更专业的技术支持服务付费。服务的售卖可以是开源项目的原主体，也可能是其他主体。当然一般来说单纯售卖服务很难支撑一个公司的发展，一般来说企业会采取混合的商业模式，例如Redhat公司。&lt;/p>
&lt;p>&lt;strong>软件再分发&lt;/strong>，企业通过自身的开发，扩展、丰富上游开源项目，并形成商业化的产品进行再分发与销售。这是一种比较常见的方法，例如围绕kubernetes存在大量的商业发行或企业产品的再整合。&lt;/p>
&lt;p>&lt;strong>直接代码售卖&lt;/strong>，开源的定义容许你直接销售源码。在互联网早期，通过收集、分发代码或二进制软件并灌制为光盘等形式来获取收益。依靠直接销售代码在如今显然不太容易成功。&lt;/p>
&lt;p>&lt;strong>双重许可/核心开放&lt;/strong>，开源项目的主体在公开源代码的同时拥有商业License版本的软件，一般来说两者会存在一些功能性差异。开源产品会拥有主要的核心功能，但是一些对企业生产级部署时需要的额外功能会通过商业版本进行销售。这类企业往往也会同时附加售卖服务。NGINX开源与NGINX Plus便是一种双重许可。NGINX Plus完全基于开源OSS，但增加了诸多企业级的特性，使得用户可以更好的在生产环境下部署运行。&lt;/p>
&lt;p>&lt;strong>SaaS&lt;/strong>，将开源软件部署为云上的SaaS服务模型，也是当前比较流行的方式。也是云时代下，开源类产品更易获得商业成功的模式。MangoDB的SaaS服务即为这种模式。当然也因为云上服务引发了一些开源的争议，在后文的风险与挑战部分再做探讨。&lt;/p>
&lt;p>&lt;strong>生态伙伴&lt;/strong>，以生态伙伴的方式进行商业化属于一种混合的商业方式。一些企业通过员工全职参与到一些非常知名及流行的开源项目中，类如Istio，Kubernetes等。对这些上游项目进行贡献，并进入到这些上游项目的技术委员会，SIG等，在社区与领域上形成较高的影响力。公司本身则会进行相关专业服务支持、增值产品等的售卖。这是一种较为高级的商业形式，一般来说多存在于大型公司或极具创新的初创公司。如 Solo，Tetrate这类公司。F5 NGINX对社区k8s Ingress Controller的贡献也是这样的模式。&lt;/p>
&lt;p>开源的商业模式还有很多，例如捐助、众筹、周边品牌等。无论哪种方式，从用户角度来说，产品必须具备价值才能够被买单。对依赖开源进行商业化服务的公司来说，则必须提供有价值的产品、有良好体验的服务，回归到产品本源才能更容易实现开源商业化。仅依赖市场营难以持久实现开源商业化。&lt;/p>
&lt;h3 id="开源的风险与挑战">开源的风险与挑战&lt;/h3>
&lt;p>开源对企业或用户拥有诸多的好处。但在实际使用过程中，依然会存在一定的风险与挑战。&lt;strong>从风险角度来说&lt;/strong>，有以下四个方面：
&lt;strong>License风险&lt;/strong>，这可能是在使用开源中首要进行考虑的方面。一般来说自由软件License是copyleft模式的，这类License一般较为严格，会强制下游用户继续保持开源。因此要小心在修改代码后是否会违反License的要求，比如HAproxy的GPL License就要小心应对，这是典型的copyleft型许可，在分发修改并编译的二进制时，必须要附上修改后的源码，利用HAproxy实现的具有交付式能力的封装产品，在交付提示符下还需要有相关简短版权声明等。尽管如今的一些开源软件License已不再要求用户必须再开源修改后的代码，但是仍然需要小心License的许可场景以及限制性要求。比如，是否将开源代码用在了被禁止的使用场景里，Redis的RSAL就对使用的场景如搜索引擎、流处理引擎等进行了限制。在使用开源软件构建云上服务能力时，对于使用Affero GPL或SSPL协议的开源产品则必须小心，他要求必须公开相关源码，这对很多类似公有云的企业非常不友好，因此Google在内部严格拒绝在公有云上使用AGPL的软件。&lt;/p>
&lt;p>&lt;strong>项目持续性风险&lt;/strong>，由于开源的主体参差不齐，有些项目是一些企业KPI导向的结果，有些则是个人基于兴趣爱好或者工作阶段的成果(可以称之为：顺手开源)。这些项目可能无法具有持久性，其生命周期也较短。如果企业选择了这类项目，则必须要充分理解此类风险，企业需要自己能够持续性开发和维护。持续性风险，还可能表现在一些地缘问题上。尽管开源倡议组织（OSI）在开源的10项定义里提到“不歧视个人或群体”，但是在一定的情况下依然会遇到这样的担忧。有时候这类担忧是因为个人情感或基于个人认知而做出的。如前段时间F5关于停止F5俄罗斯办公室职员工作的声明，让少部分人从情感上无法接受，误解为NGINX源自俄罗斯而F5却停止了俄罗斯人对NGINX的贡献。实际上这仅仅是F5基于地域战争对F5内部职员安全考虑而做出的决定。NGINX的源代码一直托管在github以及mercurial多个服务系统上，来自全世界互联网用户的贡献、访问、下载均未被影响。&lt;/p>
&lt;p>&lt;strong>项目质量风险&lt;/strong>，开源项目由于追求开放性，以及开发者经验水平并不完全一致，这可能导致代码测试不充分、代码存在安全隐患，以及不全面的使用用例等。一些大型、热门的开源项目往往会通过规范开发者贡献、设置专门测试人员、文档人员、安全小组等方式来提升软件质量。但并不是所有项目都能具有这样的资源和能力。企业在做开源项目引入时候应做充分的调研与测试。&lt;/p>
&lt;p>&lt;strong>开源嵌套风险&lt;/strong>，此类风险需从两个角度考虑。一是License，当在自身的项目中引用其它开源项目时候要注意引用方法与License的关系，注意自身项目License与引用项目License是否存在兼容风险。二是注意连环嵌套带来的质量失控，当A项目引用B，B又引用C等这类多层引用时，要综合判断上述提到的所有风险情况。&lt;/p>
&lt;p>&lt;strong>从挑战角度来说&lt;/strong>，其涵盖要宽广的多。一般来说，企业的数字化转型会涉及文化、技术、流程这三个方面。同样的，企业使用开源的挑战也可以从这三个方面来思考。
文化方面，企业建立开源文化动力不足是个挑战。企业往往会强调运行的稳定，无论是流程还是KPI考核对于IT系统往往追求的是稳定性。从项目的招标、测试、上线、运维无一不以稳定为第一。在这样的文化下，技术系统、人员思想容易僵化：近乎静态稳定的IT系统限制了IT系统对业务创新的支持，导致业务上线速度缓慢；技术人员则会依赖商业产品的技术支持，导致缺乏创新精神。开源文化动力不足，还表现在另一方面，就是企业缺乏开源贡献精神，只是一味索取使用，不对开源上游进行贡献，更害怕把自己的创新共享到社区。&lt;/p>
&lt;p>&lt;strong>技术方面&lt;/strong>，涉及两个相互关联的挑战：开源技术掌握的能力，以及掌握技术的人才。开源产品，往往不是开箱即用，需要企业根据自身情况进行再开发，同时为了实现业务能力，往往还需要采用多种开源技术。这就需要企业需要具有更多的开发人才，更多的具有创新能力的人才，需要这些才来分析研究这些开源产品，吃透并掌握核心技术。这对于采用外包模式的企业，往往挑战很大，这类企业缺少足够的人才来掌握这些技术。即便是对于具有一定开发规模的企业来说，如何转型人才，建立良好的人才上升通道，留住掌握关键开源技术、吃透开源技术的高端人才，其实也是非常大的挑战。&lt;/p>
&lt;p>&lt;strong>流程方面&lt;/strong>，企业需要将已有流程与开源相适应，这往往比较困难。使用开源技术意味着面向闭源软件的流程需要改造并适应开源技术带来的变化，无论是商务流程、资产管理、技术配套、治理流程等等。我们见到有一些企业，当使用开源后，如何购买与开源相关的技术支持服务时，在流程上仍会遇到一些问题。&lt;/p>
&lt;h3 id="开源的选择与治理">开源的选择与治理&lt;/h3>
&lt;p>开源主体和开源用户对开源的选择与治理的思考会在两个不同的范畴。因为这是两个方向的问题，尽管两者角色有时候会交叉。&lt;/p>
&lt;p>&lt;strong>开源选择，对于开源主体&lt;/strong>来说，首先要对自己的项目是否开源做出决定。项目的负责人或公司需要基于对开源的认知理解，结合所在行业的特点、领域竞争分析，自身实际情况去思考，综合分析开源或不开源将会产生的影响。当决定开源后，还需决定是企业内部开源还是对外开源。企业内部开源是一些头部互联网或技术领先的大型企业的一种做法，其本质是利用开源的好处促进企业内部创新，打破企业部门壁垒，提高企业生产力和协作效率。一般来说，企业的内部开源会经历个人或小组自发阶段，进而到企业设立内部开源管理部门进行企业全局层面的统一开源管理与协调阶段。&lt;/p>
&lt;p>当正式决定开源后，随后就需转入对开源治理的思考。对开源主体来说，开源治理包含了两个方面，一是项目工程的治理，二是社区治理。&lt;/p>
&lt;p>（1）项目工程治理，无论如何，开源项目最终还是一个软件工程，只是它是建立在更大范围内的协作、信任与贡献的基础上。因此对于项目工程质量、代码方面的管理一样适用于开源项目。需要考虑项目的范围管理、进度管理、质量管理、变更控制等，通过这些管理保证代码的质量、进度、安全等。我们可以看到一些开源项目会给出开发者指导、Code of conduct等，这些都是用来保证代码质量。同时，还需要构建良好的DevOps自动化流水线，以便让开发者的贡献过程更加顺畅，确保提交的代码经过相关review、自动化检查、自动化测试等并及时反馈结果给贡献者。此外，还需要做好风险管理，我们可以看到很多成熟的开源项目都会要求开发者签订开发者贡献协议（CLA）之后才可以提交PR，这些都是为了防范版权、非原创等方面的风险。风险管理还包含对项目中所用到的其它开源组件的License合规性管理，避免引入风险，类似FOSSA类工具往往会被用于此方面的管理。&lt;/p>
&lt;p>（2）项目社区治理，开源项目最大的特点是去中心化，人员多样化，他们来自世界各地，背景不同、文化不同。因此社区的治理本质上是对“干系人”的管理，人的管理是开源项目中最大的挑战。可以说社区治理的好不好是项目能否成功的一个非常关键的因素，因而Apache基金会特别强调“Community over code”理念。社区从一开始就应该定下明确的规则与基调，这样可确保社区始终聚拢那些具有相同认知的人。社区的治理会涉及到选择哪种治理模式，比如基金会托管还是自我管理。一般来说，加入基金会有利于项目的运作，因为这些专业的开源基金会可以帮助指导项目的运作，也可以帮助项目快速走向全球化，帮助构建项目生态，通过基金会的力量形成不同项目之间的交叉支持。当然，基金会还会运作很多活动，这些都可以帮助项目提高知名度，吸引更多开发者成为用户，并最终成为贡献者。自我管理则需要项目所有者或背后所属的组织具有较强的社区管理能力。比如NGINX就属于自我管理，F5公司通过专业的社区运营团队来治理社区。无论哪种治理方式，其核心都是要帮助项目走在正确的轨道和方向上，保证项目的可持续发展，解决项目进行过程中的各类干系人问题。社区治理需要进行的工作可包含产品布道、活动运营、开发者关系维护、Issues/PR管理，许可证管理，社区契约与规则管理、文档管理、生态构建、法律事务等。&lt;/p>
&lt;p>&lt;strong>开源选择，对开源用户来说&lt;/strong>，特别是对企业用户，开源选择的第一条就是应建立准入控制，企业可以考虑建立开源软件资产白名单，避免开发者随意引入开源软件或项目，同时对采购的服务商所提供的软件也要执行相关白名单检查。尽管这在一定程度上增大了成本，但是对于企业的安全风险控制确是非常必要的。企业应对开源项目进行充分的调研与分析，了解项目的状态、活跃度、背后的支撑力量、运作模式、用户基数、贡献者热度、License限制、技术路演、软件架构、代码质量等，并做好充分的引入前测试。还应客观引入开源项目，避免少部分人的技术情怀或倾向而导致引入“关系型开源项目”。&lt;/p>
&lt;p>再来看&lt;strong>开源治理&lt;/strong>，如同上文关于使用开源的挑战，企业可以从文化、技术、流程方面进行开源治理。&lt;/p>
&lt;p>（1）文化方面，企业应建立崇尚开源、敬畏开源的文化。在积极鼓励采用开源技术的同时，加强员工对开源的认知。如尊重版权，避免法律风险。认识开源并不等于免费，使用开源不等于取巧。认识开源并不意味着就可以自主可控。开源不等于定制化，任何有利于产品的修改与增强都应回馈到上游。企业应根据自身实际情况客观评价当前对开源的把控能力，不宜冒进。比如企业可能是需要一步一步塑造开源文化基因，这时企业更适宜采用有专业支持服务的开源软件使用模式，通过引入第三方或开源服务商的支持来帮助企业避免技术风险，实现务实的自主可控。以软负载类产品为例，企业可通过引入NGINX支持服务来尝试开源实践。对于刚进行开源实践转型的部门，如运维部门，可考虑采用商业产品+厂商开源扩展的解决方案，确保在风险可控的前提下，逐步进入开源运维。&lt;/p>
&lt;p>（2）技术方面，在开发环节建立良好的开发构建与测试平台以及安全测试平台，对相关开源代码展开代码扫描检查。识别相关库的依赖关系，发现代码本身以及关联依赖的潜在漏洞。通过开源License管理软件检查可能存在的License合规性问题以及交叉问题。在交付运行环节，使用额外的安全设备或策略对运行开源组件的环境进行安全加强，比如相关开源软件在历史上是否曾暴露过漏洞，其所依赖的组件是否暴露过漏洞，有针对性的进行加固。加强技术团队对开源技术的学习与技能提升，建立专业的开源技术组件支持团队等。&lt;/p>
&lt;p>（3）流程方面，企业可以考虑建立从引入、开发、交付、运维到退出的全流程机制来进行开源管理。从组织机制、管理制度等方面来形成开源软件引入规范、开发规范、部署规范、运维规范、退出管理等规范。引入规范可以结合上述提到的开源选择部分识别开源准入，建立准入条件，把好入口第一道关。开发规范则可以考虑定义开源软件代码的使用语言、范式、边界、修改流程、文档流程等。部署规范可围绕交付、依赖管理、安全加固、标准化环境等方面进行考虑。运维规范可考虑开源软件的运维工具、排错流程、最佳实践等方面。此外，企业还应形成闭环的管理体系，针对开源的使用与运行，建立识别与检查机制。例如识别已经在运行的开源软件及相关项目，并对其进行评价，针对发现的问题进行相关文化、流程、技术方面的修正，也要及时退出运行不佳的开源软件，确保对开源的治理始终处在有效的轨道上，避免开源的蔓延与失控。&lt;/p>
&lt;h3 id="总结">总结&lt;/h3>
&lt;p>可以看出，无论是开源主体，还是开源用户，围绕开源的理解、选择、风险、挑战、治理都是一个系统化的工程与能力。近些年，在国家大力推进开源的战略背景下，国内又提出了“可信开源”的理念，其主要目标依然是如何更好的发挥开源的作用，避免开源中可能遇到的风险。不管怎样，如果一个开源项目能够始终坚持初心，坚持以用户为中心，那么在很多方面可以消除用户在开源方面的担忧与风险。正如F5中国总经理张毅强在“2022 F5多云应用服务科技峰会”上所说：以用户需求为中心，围绕产品特性与社区平台构建开源生态。这是保证NGINX在中国可以走的更长远的核心，是F5为用户提供价值的核心，是用户信赖F5的核心。我想，这同样也适用于我们对其他开源的理解。&lt;/p></description></item><item><title>Analysis and Thinking of Enterprise Open Source</title><link>http://linjing.io/post/nginx-book-copy/</link><pubDate>Mon, 27 Jun 2022 00:00:00 +0000</pubDate><guid>http://linjing.io/post/nginx-book-copy/</guid><description>&lt;blockquote>
&lt;p>In recent years, the role of open source software in promoting the digital transformation of enterprises has become more and more obvious. The country has written the construction and improvement of the open source ecosystem into the &amp;ldquo;14th Five-Year Plan&amp;rdquo;. At the same time, topics such as open source governance and secure supply chain have become a hot topic in the industry. As open source users or as open source subjects, how should we understand open source, how to understand the community, and how to participate in open source better and more safely. This article will discuss with you the meaning of open source, business models, risks and challenges, choices and governance.&lt;/p>
&lt;/blockquote>
&lt;h3 id="what-open-source-means-for-business">What Open Source Means for Business&lt;/h3>
&lt;p>Let us first think about the meaning of open source and individuality from the perspective of an individual. We can summarize &amp;ldquo;selfishness and altruism&amp;rdquo; in one sentence. Using open source projects or products can help individuals quickly solve problems and challenges at work, and individuals can also gain innovative ideas and experience from open source projects or products. These are all selfish. When an individual is not satisfied with just being an open source user, and begins to participate in the contribution of an open source project, or even set up an open source project himself, it becomes altruistic, and other users will benefit from his contribution or project.&lt;/p>
&lt;p>The same goes for businesses. When enterprises focus on their own digital transformation, it means that enterprises will pursue a more open organizational spirit. The core of digital transformation is to use digital technology and capabilities to reengineer enterprise processes, thereby driving business model innovation, enhancing enterprise competitiveness, and even changing an industry, just like &amp;ldquo;DiDi&amp;rdquo;. This requires the enterprise&amp;rsquo;s IT infrastructure and technology to not only support the rapid changes and development of the business, but also drive business innovation. Enterprises must quickly establish business, quickly iterate business, and quickly obtain business feedback. Therefore, having an IT architecture and technology that can flexibly support short-term and long-term business development is the key. Enterprises must adopt open and collaborative new technologies to continuously improve and enhance the iterative capability of technical architecture. These all mean being &amp;ldquo;fast&amp;rdquo; enough and &amp;ldquo;innovative&amp;rdquo; enough. Open source technology just responds to the needs of enterprises, because open source technology pursues the spirit of openness, collaboration and contribution. Enterprises adopting open source technology can obtain contributions from outstanding talents from all over the world, and can rapidly improve and enhance the competitiveness of enterprises through the contributions of talents from all over the world. The open ecosystem created by open source brings this value to all parties involved.&lt;/p>
&lt;p>Take the smart car industry as an example. Traditionally, car manufacturers have relied on a large number of third-party suppliers and technologies. However, when companies enter the new smart car track, car companies need to continuously innovate vehicle functions, improve user experience, and quickly occupy the market. If car companies still adopt the traditional supplier model, they will be constrained by those closed suppliers and will not be able to innovate products and quickly occupy the market. By adopting open source technologies, such as AGL (Automotive Grade Linux), it is possible to eliminate the need to develop the basic operating system of the vehicle, and instead invest valuable R&amp;amp;D technology on more competitive innovations.&lt;/p>
&lt;p>Enterprises with an open spirit will not only stop at adopting open source technologies, but will also contribute to open source. Large enterprises can help enterprises realize market strategies, occupy the market or change the game rules of the market by open-sourcing their own products or technologies, such as Google&amp;rsquo;s Android system. Innovative small enterprises can accumulate huge community users, increase their influence, and expand their own valuations through open source.&lt;/p>
&lt;p>It can be seen that the significance of open source to the digital transformation of enterprises is obvious. On the one hand, open source is conducive to stimulating technological innovation of enterprises. Compared with the use of closed source software, open source technology makes users more creative and innovative; on the other hand, the use of open source technology can help enterprises save costs, so that more IT investments are used to deploy new technologies and accelerate digital transformation. This finding is echoed in a survey of global IT leaders on the state of open source in the enterprise 2020, with 95% of respondents saying open source is strategically important to their overall software strategy. In China, some leading Internet companies have also established special corporate open source committees to help companies achieve better open source strategies. The country has written the construction and improvement of the open source ecosystem into the &amp;ldquo;14th Five-Year Plan&amp;rdquo;.&lt;/p>
&lt;p>Open source is sometimes given the concept of &amp;ldquo;trusted and controllable&amp;rdquo;. It is true that open source cannot be directly equated with &amp;ldquo;trustworthy and controllable&amp;rdquo;. Only when an enterprise has the ability and resources to thoroughly understand and master it through learning can it be truly trusted and controllable. However, open source, in the form of open source code, allows users to be reassured and solve problems in operation and maintenance by studying open source code when necessary.&lt;/p>
&lt;p>F5 operates open source NGINX in China and deeply understands the demands of Chinese users. The reliability and ease of development of NGINX technology ensure that enterprises can have a better and more reliable IT infrastructure, which is of great significance to the digital transformation of enterprises. Whether in distributed or microservice architecture, NGINX technology has been proven by users all over the world, and its reliability ensures the stability of IT infrastructure. In China, we can see that a large number of users use NGINX technology in production, whether it is an Internet company or a financial enterprise, such as the open banking of Agricultural Bank, the business access scheduling of Xinwang Bank, and so on. The ease of development of NGINX makes these technological innovations possible. It can be said that many customers have realized the autonomous control of soft loads with the help of NGINX. Further, by localizing open source NGINX and providing open source subscription services in China, it means that Chinese users can achieve more reliable product use through local services. This service includes basic technical support and advanced expert services. Creation, development, etc.&lt;/p>
&lt;h3 id="open-source-business-model">Open source business model&lt;/h3>
&lt;p>There are many business models for open source. According to the origin, background, appeal, and subject of the project, it can generally be divided into:
advertising mode, which can obtain certain benefits by placing sponsored advertisements on the website, installation process, documents, etc. Games and search projects are generally easy to take this approach.&lt;/p>
&lt;p>&lt;strong>Selling services&lt;/strong> is a typical open source business model. The service provider itself does not provide code sales, but services, and users pay for better and more professional technical support services. The sale of services can be the original subject of the open source project or other subjects. Of course, it is generally difficult to support the development of a company by simply selling services. Generally speaking, companies will adopt a mixed business model, such as Redhat.&lt;/p>
&lt;p>&lt;strong>Software redistribution&lt;/strong> , enterprises expand and enrich upstream open source projects through their own development, and form commercial products for redistribution and sales. This is a relatively common approach, such as there is a large number of commercial distributions or reintegration of enterprise products around kubernetes.&lt;/p>
&lt;p>&lt;strong>Direct code sale&lt;/strong> , the definition of open source allows you to sell source code directly. In the early days of the Internet, revenue was made by collecting, distributing code or binary software and making it on CD-ROM. Relying on direct sales codes is obviously not easy to succeed today.&lt;/p>
&lt;p>&lt;strong>Dual license/open core&lt;/strong> , the main body of the open source project has a commercial license version of the software while publishing the source code. Generally speaking, there will be some functional differences between the two. Open source products will have the main core functionality, but some additional functionality needed for production-level enterprise deployments will be sold in commercial versions. Such companies often also sell additional services at the same time. NGINX Open Source and NGINX Plus are a dual license. NGINX Plus is completely based on open source OSS, but has added many enterprise-level features, so that users can better deploy and run in the production environment.&lt;/p>
&lt;p>&lt;strong>SaaS&lt;/strong> , deploying open source software as a SaaS service model on the cloud, is also a popular way at present. It is also a model in which open source products are more likely to achieve commercial success in the cloud era. MongoDB&amp;rsquo;s SaaS service is this model. Of course, some open source controversies have arisen because of cloud services, which will be discussed later in the Risks and Challenges section.&lt;/p>
&lt;p>&lt;strong>Eco-partnership&lt;/strong> , commercialization in the form of an eco-partner is a hybrid business approach. Some companies have full-time employees involved in some very well-known and popular open source projects, such as Istio, Kubernetes, etc. Contribute to these upstream projects and enter the technical committees, SIGs, etc. of these upstream projects, forming a high influence in the community and field. The company itself will sell related professional service support, value-added products, etc. This is a more advanced form of business, generally found in large companies or very innovative start-ups. Companies such as Solo and Tetrate. F5 NGINX&amp;rsquo;s contribution to the community k8s Ingress Controller is also in this pattern.&lt;/p>
&lt;p>There are many open source business models, such as donations, crowdfunding, and peripheral brands. Either way, from the user&amp;rsquo;s perspective, the product must have value in order to be paid for. For companies that rely on open source for commercial services, they must provide valuable products and services with good experience, and return to the origin of the product to achieve open source commercialization more easily. It is difficult to achieve sustainable open source commercialization only by relying on the market.&lt;/p>
&lt;h3 id="open-source-risks-and-challenges">Open Source Risks and Challenges&lt;/h3>
&lt;p>Open source has many benefits for businesses or users. However, in the actual use process, there will still be certain risks and challenges. &lt;strong>From the risk point of view&lt;/strong> , there are the following four aspects:
&lt;strong>License risk&lt;/strong> , which may be the first consideration in the use of open source. Generally speaking, free software licenses are in copyleft mode. Such licenses are generally stricter and will force downstream users to keep open source. Therefore, be careful whether you will violate the license requirements after modifying the code. For example, the GPL License of HAproxy should be handled carefully. This is a typical copyleft license. When distributing the modified and compiled binary, you must attach the modified source code. Packaged products with delivery capabilities implemented using HAproxy also need a short copyright statement at the delivery prompt. Although some open source software licenses today no longer require users to open source the modified code, it is still necessary to be careful about the license scenarios and restrictive requirements. For example, whether open source code is used in prohibited usage scenarios, Redis&amp;rsquo;s RSAL restricts usage scenarios such as search engines and stream processing engines. When using open source software to build cloud service capabilities, you must be careful about open source products using the Affero GPL or SSPL protocol. He requires the relevant source code to be disclosed, which is very unfriendly to many companies similar to public clouds, so Google strictly refuses it internally. Use AGPL&amp;rsquo;s software on the public cloud.&lt;/p>
&lt;p>&lt;strong>Project continuity risk&lt;/strong> , due to the uneven main body of open source, some projects are the results of some enterprise KPI-oriented, and some are the results of individuals based on hobbies or work stages (it can be called: open source smoothly). These items may not be durable and have a short life cycle. If an enterprise chooses such a project, it must fully understand such risks, and the enterprise needs to be able to develop and maintain it continuously. Persistent risks may also be manifested in some geopolitical issues. Although the Open Source Initiative (OSI) mentions &amp;ldquo;non-discrimination against individuals or groups&amp;rdquo; in its 10 definitions of open source, such concerns may still be encountered under certain circumstances. Sometimes these concerns are based on personal emotions or based on personal perception. For example, some time ago, F5&amp;rsquo;s statement on stopping the work of F5&amp;rsquo;s Russian office staff made it emotionally unacceptable to a small number of people, misunderstanding that NGINX originated in Russia, but F5 stopped Russians&amp;rsquo; contributions to NGINX. In fact, this is just a decision made by F5 based on the regional war for the safety of F5&amp;rsquo;s internal staff. The source code of NGINX has been hosted on github and mercurial multiple service systems, and contributions, access, and downloads from Internet users around the world have not been affected.&lt;/p>
&lt;p>&lt;strong>Project quality risk&lt;/strong> , open source projects pursue openness, and developers&amp;rsquo; experience levels are not completely consistent, which may lead to insufficient code testing, code security risks, and incomplete use cases. Some large and popular open source projects often improve software quality by standardizing developer contributions, setting up special testers, documentation personnel, and security teams. But not all projects have such resources and capabilities. Enterprises should do sufficient research and testing when introducing open source projects.&lt;/p>
&lt;p>&lt;strong>Open source nesting risks&lt;/strong> , such risks need to be considered from two perspectives. One is the license. When citing other open source projects in your own project, you should pay attention to the relationship between the reference method and the license, and pay attention to whether there is a compatibility risk between the license of your own project and the license of the referenced project. The second is to pay attention to the loss of quality control caused by serial nesting. When project A cites B, and B cites C and other multi-level references, it is necessary to comprehensively judge all the above-mentioned risks.&lt;/p>
&lt;p>&lt;strong>From a challenge perspective&lt;/strong> , its coverage is much broader. Generally speaking, the digital transformation of enterprises involves three aspects: culture, technology and process. Likewise, the challenges of enterprise use of open source can also be considered from these three perspectives.
Culturally, the lack of motivation for companies to build an open source culture is a challenge. Enterprises often emphasize the stability of operation. Whether it is a process or KPI assessment, stability is often pursued for IT systems. From the bidding, testing, launching, and operation and maintenance of the project, stability is the first. In such a culture, the technical system and personnel ideas tend to become rigid: the almost static and stable IT system limits the IT system’s support for business innovation, resulting in slow business launch; technical personnel will rely on the technical support of commercial products, resulting in a lack of innovation Spirit. On the other hand, the lack of motivation for open source culture is that enterprises lack the spirit of open source contribution. They only ask for use, do not contribute to the upstream of open source, and are even more afraid to share their innovations with the community.&lt;/p>
&lt;p>&lt;strong>On the technical side&lt;/strong> , there are two interrelated challenges: the ability to master open source technology, and the talent to master the technology. Open source products are often not ready to use out of the box. Enterprises need to re-develop according to their own conditions. At the same time, in order to achieve business capabilities, they often need to adopt a variety of open source technologies. This requires enterprises to have more development talents and more innovative talents, who need these talents to analyze and study these open source products, understand and master core technologies. This is often a big challenge for companies that adopt outsourcing models, and such companies lack enough talents to master these technologies. Even for enterprises with a certain development scale, how to transform talents, establish a good talent promotion channel, and retain high-end talents who master key open source technologies and understand open source technologies are actually very big challenges.&lt;/p>
&lt;p>&lt;strong>In terms of processes&lt;/strong> , companies need to adapt existing processes to open source, which is often difficult. Using open source technology means that processes oriented towards closed source software need to be transformed and adapted to the changes brought about by open source technology, whether it is business processes, asset management, technical support, governance processes, etc. We have seen that some enterprises still encounter some problems in the process of purchasing open source-related technical support services after using open source.&lt;/p>
&lt;h3 id="open-source-selection-and-governance">Open Source Selection and Governance&lt;/h3>
&lt;p>Open source subjects and open source users think about open source selection and governance in two different categories. Because it&amp;rsquo;s a question of two directions, even though the roles of the two sometimes intersect.&lt;/p>
&lt;p>&lt;strong>Open source choice, for open source subjects&lt;/strong> , the first thing to do is to make a decision on whether or not to open source their projects. The person in charge of the project or company needs to think about it based on the cognitive understanding of open source, combined with the characteristics of the industry, the analysis of competition in the field, and its own actual situation, and comprehensively analyze the impact of open source or not open source. When you decide to open source, you also need to decide whether to open source internally or externally. Intra-enterprise open source is a practice of some leading Internet or technology-leading large enterprises. Generally speaking, the internal open source of an enterprise will go through the spontaneous stage of individuals or groups, and then to the stage of unified open source management and coordination at the global level of the enterprise by setting up an internal open source management department in the enterprise.&lt;/p>
&lt;p>When the official decision to open source is made, then it is necessary to turn to thinking about open source governance. For open source entities, open source governance includes two aspects, one is the governance of project engineering, and the other is community governance.&lt;/p>
&lt;p>(1) Project engineering governance. In any case, an open source project is ultimately a software engineering, but it is based on a larger scope of collaboration, trust and contribution. Therefore, the management of project engineering quality and code is also applicable to open source projects. It is necessary to consider the scope management, schedule management, quality management, change control, etc. of the project, and ensure the quality, progress, and safety of the code through these managements. We can see that some open source projects will give developer guidance, Code of conduct, etc., which are used to ensure code quality. At the same time, it is also necessary to build a good DevOps automation pipeline to make the contribution process of developers smoother, to ensure that the submitted code undergoes relevant reviews, automated inspections, automated tests, etc., and the results are fed back to contributors in a timely manner. In addition, risk management needs to be done well. We can see that many mature open source projects require developers to sign a Developer Contribution Agreement (CLA) before submitting PR. These are to prevent copyright, non-original and other risks. Risk management also includes license compliance management for other open source components used in the project to avoid introducing risks. Tools like FOSSA are often used for this management.&lt;/p>
&lt;p>(2) Project community governance. The biggest feature of open source projects is decentralization and diverse personnel. They come from all over the world, with different backgrounds and cultures. Therefore, the governance of the community is essentially the management of &amp;ldquo;stakeholders&amp;rdquo;, and the management of people is the biggest challenge in open source projects. It can be said that the quality of community governance is a very critical factor for the success of the project, so the Apache Foundation particularly emphasizes the concept of &amp;ldquo;Community over code&amp;rdquo;. A community should have clear rules and a tone from the start, which ensures that the community always brings together those who share the same understanding. The governance of the community will involve choosing which governance model, such as foundation custody or self-management. Generally speaking, joining the foundation is beneficial to the operation of the project, because these professional open source foundations can help guide the operation of the project, and can also help the project go global quickly, help build the project ecology, and form a relationship between different projects through the power of the foundation. cross support. Of course, the foundation also runs many activities, which can help the project to increase its visibility, attract more developers to become users, and eventually become contributors. Self-management requires the project owner or the organization behind it to have strong community management capabilities. For example, NGINX is self-managed, and F5 manages the community through a professional community operation team. No matter what kind of governance method, the core is to help the project walk on the right track and direction, ensure the sustainable development of the project, and solve various stakeholder problems in the process of the project. The work required for community governance can include product evangelism, event operation, developer relationship maintenance, Issues/PR management, license management, community contract and rule management, document management, ecological construction, legal affairs, etc.&lt;/p>
&lt;p>&lt;strong>Open source selection. For open source users&lt;/strong> , especially for enterprise users, the first rule of open source selection is to establish access control. Enterprises can consider establishing a whitelist of open source software assets to prevent developers from randomly introducing open source software or projects. The software provided by the purchased service provider is also subject to relevant whitelist checks. Although this increases the cost to a certain extent, it is indeed very necessary for the enterprise&amp;rsquo;s security risk control. Enterprises should conduct sufficient research and analysis on open source projects to understand the status, activity, supporting force behind the project, operation mode, user base, contributor popularity, license restrictions, technical roadshows, software architecture, code quality, etc., and do a good job Adequate pre-introduction testing. Open source projects should also be introduced objectively to avoid the introduction of &amp;ldquo;relational open source projects&amp;rdquo; due to the technical feelings or inclinations of a small number of people.&lt;/p>
&lt;p>Let&amp;rsquo;s look at &lt;strong>open source governance&lt;/strong> . As with the challenges of using open source above, enterprises can conduct open source governance in terms of culture, technology, and process.&lt;/p>
&lt;p>(1) In terms of culture, enterprises should establish a culture of advocating and respecting open source. While actively encouraging the adoption of open source technologies, strengthen employee awareness of open source. Such as respecting copyright and avoiding legal risks. Knowing open source does not mean free, and using open source does not mean cheating. Knowing open source doesn&amp;rsquo;t mean you can be autonomous and controllable. Open source does not mean customization, and any modifications and enhancements that are beneficial to the product should be fed back to the upstream. Enterprises should objectively evaluate their current ability to control open source according to their own actual situation, and should not rashly advance. For example, enterprises may need to shape open source cultural genes step by step. At this time, it is more suitable for enterprises to adopt open source software usage models with professional support services. By introducing the support of third parties or open source service providers to help enterprises avoid technical risks and achieve pragmatic independent availability control. Taking soft load products as an example, enterprises can try open source practices by introducing NGINX support services. For departments that have just undergone open source practice transformation, such as operation and maintenance departments, they can consider adopting commercial products + manufacturers&amp;rsquo; open source expansion solutions to ensure that they gradually enter open source operation and maintenance under the premise of controllable risks.&lt;/p>
&lt;p>(2) In terms of technology, establish a good development construction and testing platform and security testing platform in the development link, and conduct code scanning and inspection on relevant open source codes. Identify dependencies of related libraries and discover potential vulnerabilities in the code itself and associated dependencies. Check for possible license compliance issues and cross-problems through open source license management software. During delivery and operation, use additional security devices or strategies to strengthen the environment in which open source components run reinforcement. Strengthen the technical team&amp;rsquo;s learning and skill improvement of open source technology, and establish a professional open source technology component support team.&lt;/p>
&lt;p>(3) In terms of process, enterprises can consider establishing a whole process mechanism from introduction, development, delivery, operation and maintenance to exit for open source management. From the aspects of organizational mechanism and management system, the open source software introduction specification, development specification, deployment specification, operation and maintenance specification, exit management and other specifications are formed. The introduction of specifications can be combined with the above-mentioned open source selection part to identify open source access, establish access conditions, and make the first pass of the entrance. The development specification can consider defining the use language, paradigm, boundary, modification process, documentation process, etc. of open source software code. Deployment specifications can be considered around delivery, dependency management, security hardening, and standardized environments. Operation and maintenance specifications can consider the operation and maintenance tools, troubleshooting processes, best practices and other aspects of open source software. In addition, enterprises should also form a closed-loop management system, and establish an identification and inspection mechanism for the use and operation of open source. For example, identify open source software and related projects that are already running, and evaluate them, make corrections in terms of culture, process, and technology for the problems found, and exit open source software that is not running well in time to ensure that the governance of open source is always Stay on track to avoid open source sprawl and runaway.&lt;/p>
&lt;h3 id="summarize">Summarize&lt;/h3>
&lt;p>It can be seen that, whether it is an open source subject or an open source user, the understanding, choice, risk, challenge, and governance of open source are all systematic projects and capabilities. In recent years, under the strategic background of the country&amp;rsquo;s vigorous promotion of open source, the concept of &amp;ldquo;trusted open source&amp;rdquo; has been proposed in China, and its main goal is still how to better play the role of open source and avoid possible risks in open source. In any case, if an open source project can always adhere to its original intention and insist on being user-centered, it can eliminate users&amp;rsquo; concerns and risks in open source in many aspects. As Zhang Yiqiang, general manager of F5 China, said at the &amp;ldquo;2022 F5 Multi-Cloud Application Service Technology Summit&amp;rdquo;: Focus on user needs and build an open source ecosystem around product features and community platforms. This is the core that ensures that NGINX can go a long way in China, the core that F5 provides value to users, and the core that users trust F5. I think the same applies to our understanding of other open source.&lt;/p></description></item><item><title>New NGINX book published!</title><link>http://linjing.io/post/nginx-book/</link><pubDate>Mon, 16 May 2022 00:00:00 +0000</pubDate><guid>http://linjing.io/post/nginx-book/</guid><description>&lt;h2 id="quick--introduction">Quick Introduction&lt;/h2>
&lt;p>Pls order it in jd.com, Chinese book name： NGINX经典教程&lt;/p>
&lt;p>&lt;img src="img/image-20220516125953377.png" alt="image-20220516125953377">&lt;/p>
&lt;p>&lt;img src="img/image-20220516130010458.png" alt="image-20220516130010458">&lt;/p>
&lt;p>&lt;img src="img/image-20220516130043488.png" alt="image-20220516130043488">&lt;/p>
&lt;p>&lt;img src="img/image-20220516124855457.png" alt="image-20220516124855457">&lt;/p></description></item><item><title>NGINX Classic Tutorial</title><link>http://linjing.io/publication/nginx-book-publish/</link><pubDate>Mon, 16 May 2022 00:00:00 +0000</pubDate><guid>http://linjing.io/publication/nginx-book-publish/</guid><description>&lt;!-- raw HTML omitted --></description></item><item><title>How an application delivery veteran see Envoy in the era of cloud native</title><link>http://linjing.io/post/f5-envoy-cloud-native/</link><pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate><guid>http://linjing.io/post/f5-envoy-cloud-native/</guid><description>&lt;h2 id="foreword">Foreword&lt;/h2>
&lt;p>Envoy, messenger, envoy, representative! Just like the meaning of the word itself, with a sense of authority, a sense of sacred full agent. Combined with its own use and role, it is really &amp;ldquo;people as their name&amp;rdquo;, can&amp;rsquo;t help but like Lyft, I don&amp;rsquo;t know which master got the name to get this name. In the current era of fiery microservices, Envoy is an absolute star, and it is no exaggeration to describe it as everyone knows. Someone once asked me how to look at Envoy and whether Envoy will replace F5 instead of NGINX in the cloud-native era. As a veteran who has experienced two waves of change in the field of application delivery technology, in this article I will talk about Envoy in the future From a perspective to understand and answer this question. Why talk a little bit, this is really not modesty, but objectively, there is really no such in-depth large-scale long-term use and research of all technical details of Envoy, so I will combine my professional experience and experience to make an Envoy Talk.&lt;/p>
&lt;h2 id="star-studded-envoy">Star-studded Envoy&lt;/h2>
&lt;p>First, let&amp;rsquo;s take a look at how Envoy officially introduced Envoy:&lt;/p>
&lt;blockquote>
&lt;p>ENVOY IS AN OPEN SOURCE EDGE AND SERVICE PROXY, DESIGNED FOR CLOUD-NATIVE APPLICATIONS&lt;/p>
&lt;/blockquote>
&lt;p>From this description on the homepage of the website, we can clearly see the official definition of Envoy, which is simply a proxy for east-west, north-south traffic in the cloud native era. Lfyt is the pioneer of the microservice application architecture. We can see Lfyt in a large number of microservice sermon articles. After a large-scale shift from monolithic applications to microservice architecture, a serious problem was placed in development. In front of the architects, on the one hand, Lyft&amp;rsquo;s services are developed in multiple languages, and the use of class libraries to solve various problems under the distributed architecture requires a lot of language adaptation and code intrusion. On the other hand, Lyft&amp;rsquo;s business Both are deployed on AWS, relying heavily on AWS&amp;rsquo; ELB and EC2, but the traffic control, insight, and problem elimination between the services provided by ELB and AWS at that time could not meet Lyft&amp;rsquo;s needs. It is based on this background, Lfyt is Envoy development started in May 2015. It was first deployed as an edge agent and began to replace ELB, and then began to be deployed as a sidecar method for large-scale deployment. On September 14, 2016, Lyft officially announced this project on its blog: Envoy C++ L7 proxy and communication bus . For a while, Envoy received a lot of attention, and companies such as Google began to contribute to this project, and donated the project to CNCF one year later in September 2017. With a good mom like Lyft, and the succession to CNCF as a rich dad, plus the half-brother Istio star brother&amp;rsquo;s blessing, it can be said that Envoy has a good scene for a while, earning enough eyeball and developer support, I graduated from CNCF in just over a year.&lt;/p>
&lt;p>Container technology has helped enterprises practice Devops and microservice transformation. The k8s container orchestration platform allows enterprises to move more business from traditional architectures to modern container-based infrastructures with more confidence. k8s solves container orchestration and applications. Issues such as publishing, but when the communication between services has changed from the previous call between memory to TCP-based network communication, the impact of the network on application services has become more huge and uncertain, based on traditional application architecture operation and maintenance The means cannot adapt and solve the huge and complex communication insights and troubleshooting between services. In order to solve such problems, the service mesh application was born and quickly became a hot topic of concern. The Istio project is the most important player in this ecosystem. Istio&amp;rsquo;s architecture is a typical management plane and data separation architecture. The choice of data plane is open, but Istio chooses Envoy as the data plane by default. The two popular stars joined forces to make Linkerd eclipsed almost at the same time. At this point in time, NGINX also briefly carried out the Nginmesh project, trying to make NGINX as the data plane of Istio, but eventually gave up at the end of 2018, why did you give up, this article will be mentioned later.&lt;/p>
&lt;p>In addition to Istio&amp;rsquo;s selection of Envoy as the data plane, there are many projects based on Envoy, such as multiple Ingress Controller projects of k8s: Gloo, Contur, Ambassador. Istio&amp;rsquo;s own Ingress gateway and Egress gateway also choose Envoy. Take a look at the Envoy users listed on their official homepage and say that starlight is not too much. Note that F5 in the list is very interesting.&lt;/p>
&lt;p>&lt;img src="envoy-endusers.jpg" alt="">
(Envoy end user list)&lt;/p>
&lt;h2 id="envoy-born-for-the-times">Envoy: born for the times&lt;/h2>
&lt;p>Below I will look at the technical aspects of why Envoy is so valued by the community. It will be summarized from the following aspects:&lt;/p>
&lt;ul>
&lt;li>Technical characteristics&lt;/li>
&lt;li>Deployment architecture&lt;/li>
&lt;li>Software Architecture&lt;/li>
&lt;/ul>
&lt;h3 id="technical-characteristics">Technical characteristics&lt;/h3>
&lt;ul>
&lt;li>Interface and API&lt;/li>
&lt;li>Dynamic&lt;/li>
&lt;li>Scalability&lt;/li>
&lt;li>Observability&lt;/li>
&lt;li>Modernity&lt;/li>
&lt;/ul>
&lt;h4 id="interface-and-api">Interface and API&lt;/h4>
&lt;p>When I first opened the configuration of Envoy, my first feeling was, God, how should such a product user configure and use. Under the intuitive experience, in an uncomplicated experimental environment, the number of lines of an Envoy&amp;rsquo;s actual configuration file actually reached 20,000 lines.&lt;/p>
&lt;pre>&lt;code># kubectl exec -it productpage-v1-7f4cc988c6-qxqjs -n istio-bookinfo -c istio-proxy -- sh
$ curl http://127.0.0.1:15000/config_dump | wc -l
% Total % Received % Xferd Average Speed Time Time Time Current
Dload Upload Total Spent Left Speed
100 634k 0 634k 0 0 10.1M 0 --:--:-- --:--:-- --:--:-- 10.1M
20550
&lt;/code>&lt;/pre>&lt;p>Although this is a dynamic configuration in the Istio environment, although there are ways to optimize it to reduce the actual configuration amount, or that we will not do such a large amount of configuration when using the static configuration method for configuration, but when we see the following actual The configuration structure output will feel that for such a software, it is obviously impractical to configure and maintain in a normal way. Its configuration is completely json structured and has a large number of descriptive configurations. Compared to NGINX and other such reverse For agent software, its configuration structure is too complicated.&lt;/p>
&lt;p>&lt;img src="envoy-json.jpg" alt="">
(Complex configuration structure)&lt;/p>
&lt;p>Obviously, Envoy&amp;rsquo;s design is not designed for manual, so Envoy designed a large number of xDS protocol interfaces, users need to design an xDS server to implement all configuration processing, Envoy supports gRPC or REST to communicate with the server to update Own configuration. xDS is the general name of the Envoy DS (discover service) protocol, which can be divided into Listener DS (LDS), Route DS (RDS), Cluster DS (CDS), Endpoint DS (EDS), and Secret DS in order to ensure consistent configuration DS-ADS of polymerization and the like, may be more xDS view here . These interfaces are used to automatically generate various specific configuration objects. It can be seen that this is a highly dynamic runtime configuration. To use it well, you must develop a server with sufficient capabilities. Obviously this is not the design thinking of traditional reverse proxy software.&lt;/p>
&lt;p>&lt;img src="envoy-xds.png" alt="">
(Picture from &lt;a href="https://gist.github.com/nikhilsuvarna/bd0aa0ef01880270c13d145c61a4af22">https://gist.github.com/nikhilsuvarna/bd0aa0ef01880270c13d145c61a4af22&lt;/a> )&lt;/p>
&lt;h4 id="dynamic">Dynamic&lt;/h4>
&lt;p>As mentioned earlier, Envoy&amp;rsquo;s configuration relies heavily on interface automation to generate various configurations. These configurations can be modified by Runtime without reloading files. In modern application architectures, the life cycle of a service endpoint becomes shorter and its operation Uncertainty or resilience has become greater, so the ability to make runtime changes to the configuration without having to reload the configuration file is particularly valuable in modern application architectures, which is an important consideration for Istio&amp;rsquo;s choice of Envoy as the data plane. Envoy also has a hot restart capability, which makes it more elegant when an upgrade or a restart is necessary, and existing connections can be protected more.&lt;/p>
&lt;p>In the Istio scenario, Envoy&amp;rsquo;s container runs two processes, one called pilot-agent and one is envoy-proxy itself. The pilot-agent is responsible for managing and starting Envoy, and generates an envoy under /etc/istio/proxy/ -rev0.json Initial configuration file, this file defines how Envoy should communicate with the pilot server to obtain the configuration, and use this configuration file to finally start the Envoy process. However, the final configuration of Envoy is not only the content in envoy-rev0.json, it contains all the dynamic configurations discovered through the xDS protocol mentioned above.&lt;/p>
&lt;pre>&lt;code># kubectl exec -it productpage-v1-7f4cc988c6-qxqjs -n istio-bookinfo -c istio-proxy -- sh
$ ps -ef
UID PID PPID C STIME TTY TIME CMD
istio-p+ 1 0 0 Jun25 ? 00:00:33 /usr/local/bin/pilot-agent proxy sidecar --domain istio-bookinfo.svc.cluster.local --serviceCluster productpage.istio-bookinfo --proxyLogLevel=warning --proxyComp
istio-p+ 14 1 0 Jun25 ? 00:05:31 /usr/local/bin/envoy -c etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster productpage.istio-bookin
istio-p+ 142 0 0 15:38 pts/0 00:00:00 sh
istio-p+ 148 142 0 15:38 pts/0 00:00:00 ps -ef
&lt;/code>&lt;/pre>&lt;p>In the envoy overall configuration dump of the following figure, you can see that the contents of bootstrap and other static and dynamic configurations are included:&lt;/p>
&lt;p>&lt;img src="envoy-dump-config-json-struc.jpg.jpg" alt="">
(Envoy configuration structure)&lt;/p>
&lt;p>Combined with the following figure, you can see the basic Envoy configuration structure and its logic, whether it is an entrance listener (similar to F5&amp;rsquo;s VS and part of the profile configuration, NGINX&amp;rsquo;s listener and some Server paragraph configuration) or routing control logic (similar to F5 LTM policy, NGINX&amp;rsquo;s Various Locations matching, etc., or Clusters (similar to F5 pool, NGINX upstream), Endpoints (similar to F5 pool member, NGINX upstream server), and even SSL certificates can be automatically discovered from the service side through the interface&lt;/p>
&lt;p>&lt;img src="envoy-basic-objects-logic.png" alt="">
(picture (From &lt;a href="https://gist.github.com/nikhilsuvarna/bd0aa0ef01880270c13d145c61a4af22">https://gist.github.com/nikhilsuvarna/bd0aa0ef01880270c13d145c61a4af22&lt;/a> )&lt;/p>
&lt;h4 id="scalability">Scalability&lt;/h4>
&lt;p>A lot of filters can be seen in the configuration of Envoy. These are the performance of its scalability. Envoy learned the architecture of F5 and NGINX, and used a lot of plug-ins to make it easier for developers to develop. From the start of listener, it supports the use of filter, and supports developers to develop L3, L4, L7 plug-ins to achieve protocol expansion and more control.&lt;/p>
&lt;p>In practice, companies may not have as many C++ development reserves as languages ​​such as JavaScript, so Envoy also supports Lua and Webassembly extensions. This aspect eliminates the need to frequently recompile binaries and restart, and on the other hand reduces enterprise plug-in development. Difficulty, so that companies can use more Webassembly compatible languages ​​for plug-in writing, and then compile to Webassenmbly machine code to achieve efficient operation. At present, Envoy and Istio are still in the early stages of using Webassembly for expansion, and it will take some time to mature.&lt;/p>
&lt;p>&lt;img src="concept-envoy-filter.png" alt="">
(Picture from &lt;a href="https://www.servicemesher.com/istio-handbook/concepts/envoy.html">https://www.servicemesher.com/istio-handbook/concepts/envoy.html&lt;/a> )&lt;/p>
&lt;p>As can be seen from the above figure, such a request processing structure is very close to the design idea of ​​the F5 TMOS system, and is similar to NGINX to a certain extent. Connections and requests correspond to different processing components at different protocol levels and stages, and these components are themselves extensible and programmable, which in turn enables flexible programming control of the data flow.&lt;/p>
&lt;h4 id="observability">Observability&lt;/h4>
&lt;p>It is said that Envoy is born with the characteristics of cloud native. One of the characteristics is the emphasis on observability. You can see the three observable components: logs, metrics, and tracing are all supported by Envoy by default.&lt;/p>
&lt;p>Envoy allows users to define flexible log formats in flexible locations in a flexible manner. These changes can be delivered through dynamic configuration to achieve immediate effect, and allows the definition of sampling of logs. In Metrics, it provides many indicators that can be integrated with Prometheus. It is worth mentioning that Envoy allows the filter itself to expand these indicators. For example, in the filter such as current limiting or verification, the plug-in itself is allowed to define its own indicators to help users better. Use and quantify the operational status of the plugin. In terms of Tracing, Envoy supports integration with third parties such as zipkin, jaeger, datadog, lightStep, etc. Envoy can produce a uniform request ID and keep it spread throughout the network structure. It also supports external x-client-trace-id to achieve A description of the relationship topology between microservices.&lt;/p>
&lt;p>&lt;img src="envoy-kiali.jpg" alt="">&lt;/p>
&lt;p>Each span generated by Envoy contains the following data:&lt;/p>
&lt;ul>
&lt;li>Set by the &amp;ndash;service-clusteroriginal service cluster information.&lt;/li>
&lt;li>The start time and duration of the request.&lt;/li>
&lt;li>Set by the &amp;ndash;service-nodeoriginal host information.&lt;/li>
&lt;li>By x-envoy-downstream-service-clusterdownstream cluster header set.&lt;/li>
&lt;li>HTTP request URL, method, protocol and user agent.&lt;/li>
&lt;li>By custom_tagsanother custom label settings.&lt;/li>
&lt;li>The upstream cluster name and address.&lt;/li>
&lt;li>HTTP response status code.&lt;/li>
&lt;li>GRPC response status and messages (if available).&lt;/li>
&lt;li>Error flag when HTTP status is 5xx or GRPC status is not &amp;ldquo;OK&amp;rdquo;.&lt;/li>
&lt;li>Track system-specific metadata.&lt;/li>
&lt;/ul>
&lt;h4 id="modernity">Modernity&lt;/h4>
&lt;p>In fact, it is obviously correct nonsense to say that Envoy has modernity. Envoy was born for modern application architecture. Here we mainly want to explain from several aspects that we can most easily feel. The first is its special structural design. In Envoy, it supports the use of iptables to intercept the traffic and do transparent processing. It can use getsockopt () to extract the original destination information in the NAT entry, and allow listeners to listen on the listener. The transferred port listener jumps to an unbound listener that actually matches the original destination information. Although from the perspective of a reverse proxy, this is a bit like F5&amp;rsquo;s VS internal jump, NGINX&amp;rsquo;s subrequest, but its biggest feature and ability lies in transparent connection, which is especially important in the deployment Pod sidecar mode, refer to specific principles herein .&lt;/p>
&lt;p>For the gray-scale publishing, traffic mirroring, circuit breaker, global current limiting and other functions that are favorite for modern applications, its configuration is also very simple. Although F5/NGINX and other software can also accomplish similar tasks, they are native Envoy has greater advantages in terms of ease of configuration and ease of configuration.&lt;/p>
&lt;p>Another manifestation of modernity is the support of the protocol. Look at the following supported protocols. Students who are familiar with application delivery and reverse proxy software may not help but express their admiration. The support of these protocols on the other hand shows Envoy’s A feature that is more oriented towards developers and SRE.&lt;/p>
&lt;ul>
&lt;li>gRPC&lt;/li>
&lt;li>HTTP2&lt;/li>
&lt;li>MongoDB&lt;/li>
&lt;li>DynamoDB&lt;/li>
&lt;li>Redis&lt;/li>
&lt;li>Postgres&lt;/li>
&lt;li>Kafka&lt;/li>
&lt;li>Dubbo&lt;/li>
&lt;li>Thrift&lt;/li>
&lt;li>ZooKeeper&lt;/li>
&lt;li>RockeMQ&lt;/li>
&lt;/ul>
&lt;h3 id="deployment-architecture">Deployment architecture&lt;/h3>
&lt;p>After understanding the technical characteristics of Envoy, let&amp;rsquo;s look at Envoy from the perspective of deployment architecture.&lt;/p>
&lt;p>Complete Sidecar model deployment, which is the biggest deployment feature of Envoy. The communication between services is completely transformed into the communication between Envoy agents, so that many non-business functions are removed from the service code to external proxy components. Envoy is responsible for network communication control Observable with flow. It can also be deployed as a simplified sidecar, which only acts as a proxy for the inbound direction of service without additional traffic manipulation. This structure is used in the external observability based on NGINX to achieve business observability
&lt;img src="t1.jpg" alt="">&lt;/p>
&lt;p>Hub type, which is the same as the Router-mesh type concept in NGINX&amp;rsquo;s MRA. All services use a centralized Envoy to communicate. This deployment structure is generally suitable for small and medium-sized services. Service flow can be directed by adapting to service registration. To Envoy
&lt;img src="t2.jpg" alt="">&lt;/p>
&lt;p>Envoy can also be used as an Ingress edge gateway or Egress gateway. In this scenario, Envoy is generally used for Ingress controller or API gateway. You can see that many such implementations like to use Envoy as the underlying layer, such as Gloo, Ambassador, etc.
&lt;img src="t3.jpg" alt="">&lt;/p>
&lt;p>The following deployment structure should be familiar to everyone. As an Edge gateway, Envoy also deploys an additional layer of microservice gateway (or proxy platform layer)
&lt;img src="t5.jpg" alt="">&lt;/p>
&lt;p>Finally, this is to integrate all forms of Envoy deployment. This architecture may be in the middle of the process of migrating services from traditional architecture to microservice architecture
&lt;img src="t4.jpg" alt="">&lt;/p>
&lt;p>Ok, take a look at how Envoy is used in Istio
&lt;img src="t6.jpg" alt="">&lt;/p>
&lt;p>In summary, due to the cross-platform nature of Envoy, it has the same flexible deployment structure as NGINX, but in fact the deployment structure often has a strong relationship with the final configuration implementation mechanism, can the software&amp;rsquo;s ability adapt to the flexibility under this structure Implementation with simple configuration is the ultimate test. Objectively speaking, Envoy has an advantage in this respect.&lt;/p>
&lt;h3 id="software-architecture">Software Architecture&lt;/h3>
&lt;p>Envoy adopts a single-process multi-thread design structure, and the main thread is responsible for configuration updates and process signal processing. Requests are handled by multiple worker threads. In order to simplify and avoid processing complexity, a connection is always handled by one thread, which can minimize some lock operations caused by data sharing between threads. Envoy avoids state sharing between threads as much as possible, and designed the Thread Local Store mechanism for this purpose. In the log writing, the worker thread actually writes to the memory cache, and finally the file refresh thread is responsible for writing to the disk, which can improve efficiency to a certain extent. Overall, Envoy is still more focused on simplifying complexity and emphasizing flexibility, so unlike NGINX, it does not put the pursuit of performance in the first place, which can be obtained in the relevant official blog of Envoy verification.&lt;/p>
&lt;p>&lt;img src="envoy-thread.png" alt="">&lt;/p>
&lt;p>Similar to NGINX, Envoy is an asynchronous, non-blocking design, using an event-driven approach. Each thread is responsible for each listener, SO_REUSEPORT can also be used to share sockets, NGINX also has a similar mechanism.&lt;/p>
&lt;p>&lt;img src="t7.jpg" alt="">&lt;/p>
&lt;p>After the listener listens and starts processing, the connection will be processed by subsequent L3, 4, 7 and other filters according to the configuration.&lt;/p>
&lt;p>&lt;img src="envoy-arch.jpg" alt="">&lt;/p>
&lt;h2 id="f5nginx-the-sword-is-not-out">F5/NGINX: the sword is not out&lt;/h2>
&lt;p>After understanding the technical characteristics and architecture of Envoy, we return to the original point of this article. Envoy has been carrying the genes of modern application architecture from birth, does it mean that these front waves such as NGINX/F5 are out of date.&lt;/p>
&lt;p>I remember the author of NGINX, Igor, at the F5 China 520 conference to explain why NGINX is so successful. He said that he did not expect to be so successful because the reason is that he developed the right software at the right time. We know that during the period around 2003, there was still no talk about distributed architecture and microservices. At that time, the main problem to be solved was stand-alone performance. Based on this background, NGINX is strict in terms of architecture design and code quality. Demanding performance. In terms of functionality, NGINX was originally a Web Server software, L7 reverse proxy is an extension of its capabilities, and L4 proxy capabilities increase even later. In view of this background, from the perspective of modern application architecture, there are indeed some Capability is more difficult to cover. Similarly, Envoy was born and developed in the era of modern application architecture. As Envoy self-explained, it refers to a large number of existing hardware and software reverse proxy and load balancing products. From the above technical analysis, it can also be seen that Envoy has many NGINX and F5 Architectural concept, it can be said that Envoy draws many essences from mature reverse proxy products, and fully considers the needs of modern application architecture when designing, it is also a correct software at the right time.&lt;/p>
&lt;p>Under the microservices architecture, many problems have become how to control the communication and traffic insights between services. This is a typical application delivery field. As a frontier in this field, on the one hand, we must actively embrace and adapt to the new era of application architecture. On the one hand Need to innovate and continue to lead new directions. There have been two technological innovations in this field in history. The first was around 2006, when the topic of &amp;ldquo;load balancing was dead&amp;rdquo; was fired. The essence was that the market began to change at that time, and everyone was no longer satisfied with simple loads. Balanced, demand is derived from more complex scenarios such as application security, network optimization, application optimization, access control, and flow control. The concept of application delivery began to be proposed. It can be said that before 2006, the main concepts and technical directions of the market were based on The four-layer switch is the core concept of load balancing technology. Most players are traditional network manufacturers. The thinking and concepts are based on network switching. F5 is like a strange guy. The product design thinking is completely on another dimension. The TMOS V9 operating system, which has been released since 2004, has led the market since then, and no one has surpassed it for 10 years thereafter. The second technological innovation occurred around 2016. Affected by the cloud and microservices, software and lightweight became the mainstream of the market. At the same time, Devops thought means that the role of users has changed. The traditional design for network operation and maintenance personnel It began to become difficult to meet market demand. The field dominated by F5 has also undergone new changes in the market. Gartner no longer publishes magic quadrant analysis in the field of application delivery, and instead forms guidance in the way of Guide.&lt;/p>
&lt;p>&lt;img src="F5-stock.jpeg" alt="">&lt;/p>
&lt;p>Looking at the present, history is always surprisingly similar.&lt;/p>
&lt;p>The modern application architecture is developing rapidly, and a large number of applications are beginning to be micro-serviced. However, from the perspective of the overall chain of business access, Envoy cannot solve all problems, such as application security protection, complex enterprise protocols, and different needs caused by different organizational relationships. It can be seen that the application delivery products represented by F5/NGINX have also begun to actively realize product integration under the Devops tide. F5 has released a complete automated tool chain, from the product’s bootstrap to network configuration, to application service configuration, to the final Monitoring and telemetry have formed a complete interface, and use declarative interface to promote product management to a higher role crowd and management system. NGINX also builds its own API and Controller plane, and provides a declarative API interface to the outside world. Developers can better use the interface to integrate into their own control plane. These changes are for developers or SRE to better use F5/NGINX. For details, please refer to my &amp;ldquo;From Traditional ADC to Cloud Native ADC&amp;rdquo; &lt;a href="https://mp.weixin.qq.com/s?src=11&amp;amp;timestamp=1593224168&amp;amp;ver=2425&amp;amp;signature=znUdlLDdpbGGxWX7pZhH2uSVq1SAdQuloO09HIXssdQ15nRtWVOIgzlYTFmjOIUsDrqghPbSZM6vQI45TIqmINQKjposI7AfJ6jKQaEXm9KD4tEV5Bk9AF0RGuKvVuHI&amp;amp;new=1">series of articles&lt;/a>.&lt;/p>
&lt;p>&lt;img src="slides-3.jpg" alt="">&lt;/p>
&lt;p>After acquiring NGINX and Shape, F5 put forward a new view that will make full use of the widely accessible data plane capabilities, and use AI to further tap the data potential to help users better grasp and understand application behavior and performance, and provide references for business operations. , And feedback to component configuration and operation management to form a closed loop.&lt;/p>
&lt;p>An important scenario for modern application delivery is still indispensable, that is, application security. Although Istio and other products have made many attempts in secure communication, identity, and strategy, application security itself is relatively lacking. F5 is a leading manufacturer in the field of WAF security Through the transfer of security capabilities to NGINX, a new NGINX APP Protect is formed, which uses its cross-platform capabilities to help users better manage application security capabilities in microservice scenarios and help enterprises better implement DevSecOps.&lt;/p>
&lt;p>If we compare the technical features of Envoy with F5, we can see that F5 lacks scalability and modernity to a certain extent. F5 has good programming control capabilities, but it is relatively larger than the development of larger plug-ins. Insufficient, this and modernity can often be linked together. For example, if you want to make a complex 7-layer filter similar to Envoy for a very new protocol, it is impossible to achieve, although iRule or iRuleLX can do something to a certain extent. However, in any case, the final product form of F5 itself determines that F5&amp;rsquo;s BIGIP cannot be completely cross-platform, because it cannot run as a container. It is worth expecting that such morphological restrictions will be broken by F5&amp;rsquo;s next-generation TMOS system.&lt;/p>
&lt;p>Service Mesh is the current popular technology direction. F5 builds an enterprise-level Aspen Mesh service mesh product based on Istio, which helps enterprises deploy and use Istio better and easier. Aspen mesh team members enter the Istio Technical Oversight Committee with only 7 positions and are responsible for the important responsibilities of Istio&amp;rsquo;s RFCs/Designs/APIs. Although Istio has absolute ecology and popularity in the field of service mesh, this does not mean that Istio is the only choice. In many cases, customers may want to adopt a more concise Service Mesh to achieve most of the required functions instead of deploying one. The entire complex Istio solution, NGINX Service Mesh (NSM) based on NGINX components will bring new choices to users, a more simple and easy to use Service Mesh product, this is the reason why we mentioned NGINX to terminate Nginmesh at the beginning of the article .&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>Technology development is an inevitable process. In 2006, it evolved from traditional load balancing technology to application delivery. In addition to load balancing, it introduced many aspects such as security, access control, access control, and flow control. Around 2016, new technological changes have occurred in this field again. The emergence of a large number of new generation reverse proxy open source software has a new impact on traditional application delivery products. Active adaptation and change and innovation are the key to winning. Envoy has excellent capabilities as a new representative, but it is not a silver bullet to solve all problems. Envoy has a steeper learning curve and higher development and maintenance costs. For enterprises, they should choose the appropriate solution and Products to solve different problems in the architecture, to avoid catching the trend and let yourself fall into the trap.&lt;/p>
&lt;p>F5 needs more to let developers understand the huge potential of TMOS system (especially the subversion of the next generation products in architecture and form), understand its excellent all-agent architecture and program control at any level, so that developers, SRE develops with F5 TMOS as a capability platform and middleware, and better utilizes F5&amp;rsquo;s own application delivery capabilities to quickly realize its own needs.&lt;/p>
&lt;p>Finally, again quote a sentence from the homepage of the official Envoy website:&lt;/p>
&lt;blockquote>
&lt;p>As microservice practitioners soon realized, most of the operational problems that arise when moving to a distributed architecture are ultimately based on two aspects: network and observability.&lt;/p>
&lt;/blockquote>
&lt;p>And to ensure more reliable network delivery and better observability is the strength of Qianlang. Innovate, Qianlang.&lt;/p>
&lt;p>Written at the end: No matter how the technology changes, the human factor is still the core, regardless of the company or the manufacturer, in such a wave of technology, it should have sufficient technical reserves, just like the traditional financial industry through the establishment of technology companies to seek transformation, Manufacturers also need to be transformed. F5 China&amp;rsquo;s SE has almost 100% passed the CKA certification. Regardless of the relative proportion or absolute number, it should be unique in the industry. The transformation is not only in products, but also in thinking.&lt;/p>
&lt;p>Check more istio practice detail at my tech blog &lt;a href="https://imesh.club">https://imesh.club&lt;/a>&lt;/p></description></item><item><title>What changes does envoy bring to ADN</title><link>http://linjing.io/publication/envoy-2020-6/</link><pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate><guid>http://linjing.io/publication/envoy-2020-6/</guid><description>&lt;!-- raw HTML omitted -->
&lt;p>Check here for full article &lt;a href="https://www.servicemesher.com/blog/thoughts-to-envoy-from-adn-perspective/">The link&lt;/a>.&lt;/p></description></item><item><title>NGINX and oAuth2/OIDC series one</title><link>http://linjing.io/post/nginx-oauth2-oidc-series/</link><pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate><guid>http://linjing.io/post/nginx-oauth2-oidc-series/</guid><description>&lt;p>Nowadays, the Internet has penetrated various life and business scenarios. Just like people have never been a simple individual in real life, we need to have many complicated relationship networks. The same is true of Internet applications, and it is now difficult to see which application can develop completely independently without having a relationship with its surroundings. Therefore, today&amp;rsquo;s applications are very particular about their own ecology, and they need to share a lot of information with each other. These complex relationships raise a very important issue: identity authentication, resource authorization, and account maintenance. Of course there is API authentication access control.&lt;/p>
&lt;p>For example, in your daily life, you may need to use dozens of apps. Each of these apps has an independent account and password. You need to maintain different accounts and passwords. You may have one set of accounts for all apps in order to save trouble. Password, so after the application A was violently vaulted, you are forced to change all the passwords of the accounts of these dozens of apps. You may also be more concerned about the security of your account, so you use a fixed + changing password to combine dozens of APP accounts, which is very good, can help alleviate a large part of security issues, and at the same time reduce your password maintenance. Question (Is the memory okay), but these apps may not be able to follow the fixed + change mode you envisioned as you wish, some may only support numbers, some support numbers plus passwords, and some still It requires a minimum length and a more complicated combination, so you start to use a small book to record the password format of different applications (well, at least there was a stage where I did this, recorded in a description language on the computer, when forgotten At that time, go to find this hint in the computer).&lt;/p>
&lt;p>Is there a better way?&lt;/p>
&lt;p>If you trust a company that has done well in security and has a good reputation system, can you use this account to run the Internet? I believe we already have the answer. Today, we may have used it a lot of times. When you log in to the XX application, you are used to skip user registration and go to click &amp;ldquo;log in with ***&amp;rdquo;, in the pop-up interface Here is a very sacred point under &amp;ldquo;Agree&amp;rdquo;. So you no longer need to remember so many application accounts. This is actually a typical open authorization (Open Authorization) referred to as oAuth (current version is 2, also known as oAuth2).&lt;/p>
&lt;p>Um, you seem to be lying to me. You have said so much, it seems to be all about authentication, why it is said to be Authorization. Yes, you are right, but I did not lie to you, but there are some silly problems that are not easy to distinguish. The original purpose of this oAuth design is to solve the problem of data access between interest groups, just like us As mentioned at the beginning, there is a problem of mutual access to a large amount of data between applications of different companies. For example, company A has developed an online photo printing application, but this company does not operate photo storage services. Your photos may exist in B, C, D. On the network disks of different companies (Yes, in order to take advantage of the early days, I did occupy a lot of the network disks of many companies. Later, some of their network disks used rogues to send a notice and couldn&amp;rsquo;t do anything. Fortunately, I used RAID1), so this caused problems. How do you send photos to this online printing company, download them from the BCD network disk and send them to them? Give the account number and password of the network disk to the printing company? Obviously these methods will not work. If you rely on downloading and uploading, it is estimated that you are too lazy to get it. If you are giving an account password, unless you are not sober.&lt;/p>
&lt;p>For printing companies and network disk service providers, they also have similar troubles. If users are allowed to upload and download, the user experience is too bad, and they also maintain a whole set of such systems. Therefore, printing companies hope that there is a simple way to connect at the same time. BCD network disk company, as long as one of the users of these network disks agrees, it will automatically pull down the user photos from these network disks to print, and own 0 inventory. For the BCD network disk company, storing cold data alone is obviously not the purpose. Moreover, you are still in piao, you have to do tricks, so the network disk company also wants to dock these companies that print pictures, but for them It is necessary to solve the user&amp;rsquo;s security issues on accounts and photos.&lt;/p>
&lt;p>So it can be seen that for these three different stakeholders, there is a desire to have something to solve their problems at the same time. This is authorization. When the user wants to print the photo, the printing company guides the customer to enter the network disk interface. The user is Log in to the network disk and authorize the network disk to allow which of my resources to be shared with the photo printing company. For example, share your beautiful photos to print, and the original photos are not allowed to be accessed by the printing company, which is very safe. So we can summarize:&lt;/p>
&lt;p>&lt;img src="https://imesh.club/upload/2020/05/1589076716809-1024x441.jpg?v=1589076733" alt="">&lt;/p>
&lt;p>oAuth is used to solve such a scenario, so you can see that it is an authorization process. But you haven’t said why it was a certification at the beginning, hmmm. After all, I also spent a lot of time to learn it. It is also a process after finishing it. Just like this article, it is a series. Only the following articles can be finished:&lt;/p>
&lt;ol>
&lt;li>NGINX as (Client) role and resource service role proxy in oAuth, Authorization code mode (with oAuth proxy service)-this article&lt;/li>
&lt;li>NGINX as (Client) role and resource service role proxy in oAuth, Authorization code mode (Without the help of oAuth proxy service)&lt;/li>
&lt;li>NGINX authenticates and recognizes id_token in OIDC (Implicit mode)&lt;/li>
&lt;li>NGINX acts as a resource service role in oAuth, proxy authentication and identity information recognition (Token introspection)&lt;/li>
&lt;/ol>
&lt;p>Why is authentication involved here? In fact, you will find that during the authorization process, the identity is obviously inescapable. The authorization must be based on a certain user, so the oAuth specification does not emphasize that you can’t do this. In addition to authorization, plus sometimes, we really do not need authorization scenarios, but want to reduce our account maintenance, use one company account to log in many products of other companies, so there are a lot of oAuth For authentication scenarios, of course, it is precisely because oAuth does not make many standardized definitions for authentication, which leads to different designs of programs of different companies when implementing authentication. There is no standard way to obtain user information. A common standard scope, based on this, OpenID Connect (OIDC) appeared. OIDC is based on oAuth. The communication process of several parties is the same. The difference is that OIDC is sending to IdP (the party that stores the account and performs verification). When the request is initiated, the openid tag will be brought in the scope. In the end, the information returned by the IdP will also carry an ID token (JWT) in addition to the oAuth normal access_token, and the application can use it as a login after getting this ID token. If you need more additional information, you can take the access_token and go to the userinfo endpoint to get more user information. In addition, OIDC is a protocol family and contains many other specifications, such as session management and registration discovery. Because oAuth and OIDC are very similar in communication mechanism, we often confuse the two. We often say that oAuth authentication should actually be oAuthZ, and OIDC is oAuthN.&lt;/p>
&lt;p>Back to this article, in this article, we will follow the interaction of a standard oAuth authorization code mode to see what NGINX can help users do here, and why NGINX is needed to do such a thing.&lt;/p>
&lt;p>First, we need to sort out the entire interaction process in the oAuth authorization code mode. In order to avoid the obscurity of RFC , let&amp;rsquo;s assume a scenario.&lt;/p>
&lt;p>You are in a startup company, such as a company engaged in AI and big data (of course, it is not listed yet, it is listed, and you may not have time to read this article), your company uses a lot of cloud service examples, buy servers to engage in computer rooms , Engage in infrastructure, that is not a thing. You have used open source to build a lot of systems, and quickly put your business online. Everyone knows that open source systems have a great feature, which is friendly to developers. What does it mean, how is it convenient (that is, developers are lazy, non- To be straight&amp;hellip;), so you see that many open source systems don’t think about authentication, and you visit it after installing it. It seems that there is no account authentication as a matter of course. At first, it didn’t matter, because you were alone, what You have to do everything, as more and more systems, employees begin to increase, you need to make some restrictions on access to different systems for different people, and you are still going to go public, as a public company, your system There is no account, so it is unreasonable. Then you have some application development systems that need to connect to the github API. You need to allow only some advanced developers to access a private repo. And you have no time to build a new user management system yourself. Fortunately, these people have github accounts, so you can use these github accounts to do the simplest and fastest things. These requirements can be summarized as:&lt;/p>
&lt;p>A function needs to be implemented on different systems to enable these systems to interface with github, and use the github account to determine whether employees can access a certain system
Log in with the github account on the application development system, and apply for resource authorization from github to include the person’s repo and other information. If the person does not have the private repo permission, the natural application development system cannot obtain the private under the permission of this employee. repo content
These requirements oAuth can help solve, but there is a problem. If you join the oAuth mechanism, you need to develop on the system. So many open source systems, the development language is different, and even some systems dare not rush to redevelop. It is actually very difficult to achieve and the workload is actually very large.&lt;/p>
&lt;p>Before looking at what NGINX can do, let&amp;rsquo;s take a look at the oAuth process without NGINX and the above requirements.&lt;/p>
&lt;p>&lt;img src="https://imesh.club/upload/2020/05/1589087602055-1024x534.jpg?v=1589087614" alt="">&lt;/p>
&lt;p>From the above process, it can be seen that for users, they log in and authorize once on github, and the browser makes two jumps. The really useful access_token is between the back-end application server and github. The user and the browser itself cannot see the content of this access_token, which is called the backend channel and is relatively safe. So what does the application do after getting this access_token?&lt;/p>
&lt;p>-If it is limited to obtaining some basic information of the user, and the returned access_token is JWT, then the application server can obtain the content in the JWT by itself, so that the user information extraction is associated with some local user IDs, which can be used as Used for login (of course, if it is pure identity authentication and this joint login scenario, in fact, OIDC should still be considered). Of course, if the access_token here is opaque, then the application server also needs to do token introspection, that is, it needs to be verified again with the authorized party before the relevant information can be used.&lt;/p>
&lt;p>-If it is not limited to obtaining user information, but to obtain additional resources, such as the need to obtain the person&amp;rsquo;s repo content, then the application server needs to access this access_token to access a github repo resource server (resource server and The authorization servers are not necessarily the same, and large-scale scenes are usually not the same) to obtain the person&amp;rsquo;s repo content, then the above picture becomes like this:&lt;/p>
&lt;p>&lt;img src="https://imesh.club/upload/2020/05/1589088979883-1024x570.jpg?v=1589089012" alt="">&lt;/p>
&lt;p>So, you will find that the web application backend is very critical. It participates in the entire oAuth process and finally obtains the access_token. Imagine, as you said at the beginning of the company, many open source are developed in different languages. System, you have to transform to add this ability. At this time, you actually only want to decide based on the user&amp;rsquo;s information that the system must be logged in through the oAuth process before it can be accessed, or the system determines who can access based on the user name.&lt;/p>
&lt;p>This work can actually be achieved by placing NGINX in front of the web application backend, which means that NGINX is allowed to participate in the oAuth authentication process on behalf of the backend application, and then NGINX can decide whether to allow or reject certain users based on access_token, or Transparently transmit user information to back-end applications for more processing.&lt;/p>
&lt;p>Carefully observe the entire verification process above, which requires NGINX to participate in the construction of the jump return, and use the authorization code to construct the request to directly access the github authorization server. If these tasks are done purely on NGINX, it is actually very difficult. Development through njs is a way but requires the ability to authenticate JWT (so NGINX Plus does not need to install an oauth proxy service like the demo in this article, It can be realized by directly using the njs module + KV module + JWT module. For details, please refer to the second part of this series), but in fact, it can be achieved with the help of the ability of auth_request and an oAuth proxy, which means that we need to be in various The implementation code of the oAuth authentication process created on the open source system is abstracted to it, and a general one is involved. The oAuth proxy agent participates in this oAuth process, and finally the obtained access_token is parsed out. The relevant claims information is returned to NGINX, NGINX Based on this information, we will control whether to allow access to a resource, or transparently pass relevant user information to the final application. So its implementation logic is as follows:&lt;/p>
&lt;p>&lt;img src="https://imesh.club/upload/2020/05/1589090989794-1024x566.jpg?v=1589091004" alt="">&lt;/p>
&lt;p>The idea and principle of implementation (the following serial number has nothing to do with the figure):&lt;/p>
&lt;ol>
&lt;li>Configure NGINX to publish protected applications&lt;/li>
&lt;li>Configure auth_request under the location section of the relevant application&lt;/li>
&lt;li>In this way, when the request reaches NGINX, NGINX will initiate the sub-request authentication by auth_request&lt;/li>
&lt;li>The sub-request will be proxy_pass to an interface of the oauth proxy service&lt;/li>
&lt;li>According to the characteristics of auth_request, it is necessary for oauth proxy to return the relevant status code to indicate whether NGINX is released or returns 401&lt;/li>
&lt;li>Therefore, after receiving the sub-request, the oauth proxy will determine whether the user has previously completed the relevant oauth authentication work. If the user has not logged in, or the validity period has expired, then the oauth proxy returns 401 (here depends on whether the user browser carries the oauth proxy Issued by a cookie information to check)&lt;/li>
&lt;li>NGINX intercepts the status of 401, and implements the definition of error_page to send a 302 jump to the user&amp;rsquo;s browser if 401 is returned. The address of this jump is actually a special interface of oauth proxy used to trigger the subsequent oAuth process. The subsequent process is no different from normal oAuth.&lt;/li>
&lt;li>After the oAuth proxy completes the entire oAuth process, it returns a 302 jump to the user browser, and this return will also carry the relevant cookie to allow it to revisit the protected application&lt;/li>
&lt;li>After NGINX receives the request, it triggers auth_request again. auth_request sends the request to an interface of oauth proxy again. This visit carries the cookie in 8. This way, the oauth proxy knows who it is based on the cookie, and resolves its access_token to pass relevant claims. Put it in the response header and return to NGINX&lt;/li>
&lt;li>Use auth_request_set to put the claims in the response header of the sub-request into variables and pass it to the parent request&lt;/li>
&lt;li>NGINX judges whether to release based on these variables, or puts these user information in the request header to pass the content to the last protected application&lt;/li>
&lt;/ol>
&lt;p>There are many implementations of such an oauth proxy online, here is a brief list:
&lt;a href="https://github.com/vouch/vouch-proxy">vouch-proxy&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://github.com/oauth2-proxy/oauth2-proxy">oauth2 proxy&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://github.com/jirutka/ngx-oauth">oauth2 proxy by lua -implement proxy directly in lua, no need to install additional proxy service&lt;/a>&lt;/p>
&lt;h2 id="demo">Demo&lt;/h2>
&lt;p>This demonstration uses NGINX plus and vouch-proxy to achieve. For the specific installation and configuration of vouch-proxy, please refer to its github directly, it is not complicated&lt;/p>
&lt;p>&lt;img src="https://imesh.club/upload/2020/05/1589093713142-1024x705.jpg?v=1589093738" alt="">&lt;/p>
&lt;p>In the actual demo, the web application backend actually has an intermediate NGINX to simulate, using return to return the content.&lt;/p>
&lt;p>NGINX configuration:&lt;/p>
&lt;pre>&lt;code>############entry for protected app http://authcode.imesh.club/personalinfo
server {
listen 80;
server_name authcode.imesh.club;
#root /var/www/html/;
# send all requests to the `/validate` endpoint for authorization
auth_request /validate;
#The location is for auth_request subrequest
location = /validate {
# forward the /validate request to Vouch Proxy
proxy_pass http://127.0.0.1:9090/validate;
# be sure to pass the original host header
proxy_set_header Host $http_host;
# Vouch Proxy only acts on the request headers
proxy_pass_request_body off;
proxy_set_header Content-Length &amp;quot;&amp;quot;;
# optionally add X-Vouch-User as returned by Vouch Proxy along with the request
auth_request_set $auth_resp_x_vouch_user $upstream_http_x_vouch_user;
# these return values are used by the @error401 call
auth_request_set $auth_resp_jwt $upstream_http_x_vouch_jwt;
auth_request_set $auth_resp_err $upstream_http_x_vouch_err;
auth_request_set $auth_resp_failcount $upstream_http_x_vouch_failcount;
}
# if validate returns `401 not authorized` then forward the request to the error401block
error_page 401 = @error401;
location @error401 {
# redirect to Vouch Proxy for login
return 302 http://vouch.imesh.club/login?url=$scheme://$http_host$request_uri&amp;amp;amp;vouch-failcount=$auth_resp_failcount&amp;amp;amp;X-Vouch-Token=$auth_resp_jwt&amp;amp;amp;error=$auth_resp_err;
# you usually *want* to redirect to Vouch running behind the same Nginx config proteced by https
# but to get started you can just forward the end user to the port that vouch is running on
}
# for the real service that being protected
location / {
# forward authorized requests to your service protectedapp.yourdomain.com
##he backend real server also simiulated by this nginx
proxy_pass http://127.0.0.1:8080;
# you may need to set these variables in this block as per https://github.com/vouch/vouch-proxy/issues/26#issuecomment-425215810
auth_request_set $auth_resp_x_vouch_user $upstream_http_x_vouch_user;
auth_request_set $auth_resp_x_vouch_idp_claims_avatar $upstream_http_x_vouch_idp_claims_avatar_url;
auth_request_set $auth_resp_x_vouch_idp_claims_company $upstream_http_x_vouch_idp_claims_company;
auth_request_set $auth_resp_x_vouch_idp_claims_blog $upstream_http_x_vouch_idp_claims_blog;
# set user header (usually an email)
proxy_set_header X-Vouch-User $auth_resp_x_vouch_user;
# optionally pass any custom claims you are tracking
proxy_set_header X-Vouch-IdP-Claims-company $auth_resp_x_vouch_idp_claims_company;
proxy_set_header X-Vouch-IdP-Claims-avatar $auth_resp_x_vouch_idp_claims_avatar;
proxy_set_header X-Vouch-IdP-Claims-blog $auth_resp_x_vouch_idp_claims_blog;
}
}
&lt;/code>&lt;/pre>&lt;p>Simulation configuration of back-end applications&lt;/p>
&lt;pre>&lt;code> server {
listen 8080;
location /personalinfo {
default_type text/html;
set $user $http_x_vouch_user;
set $avatar $http_x_vouch_idp_claims_avatar;
set $company $http_x_vouch_idp_claims_company;
set $blog $http_x_vouch_idp_claims_blog;
return 200 '&amp;amp;lt;html&amp;gt;&amp;amp;lt;head&amp;gt;&amp;amp;lt;meta http-equiv=&amp;quot;Content-Type&amp;quot; content=&amp;quot;text/html; charset=utf-8&amp;quot; /&amp;gt;&amp;amp;lt;/head&amp;gt;&amp;amp;lt;h2&amp;gt;Your personal info:&amp;amp;lt;/h2&amp;gt;&amp;amp;lt;hr /&amp;gt;Name: $user &amp;amp;lt;br&amp;gt;avatar: $avatar &amp;amp;lt;br&amp;gt;company: $company &amp;amp;lt;br&amp;gt;blog:$blog &amp;amp;lt;/html&amp;gt;';
}
}
&lt;/code>&lt;/pre>&lt;p>Responsible for receiving the request configuration initiated by the client browser to the oauth proxy:&lt;/p>
&lt;pre>&lt;code>#######work for vouch login/auth
server {
listen 80;
server_name vouch.imesh.club;
location / {
proxy_pass http://127.0.0.1:9090;
# be sure to pass the original host header
proxy_set_header Host vouch.imesh.club;
}
}
&lt;/code>&lt;/pre>&lt;p>The effect of the visit process:
&lt;img src="https://imesh.club/upload/2020/05/%E5%9B%BE%E7%89%87-1-1-653x1024.png?v=1589095217" alt="">&lt;/p>
&lt;p>&lt;img src="https://imesh.club/upload/2020/05/%E5%9B%BE%E7%89%87-2-682x1024.png?v=1589095226" alt="">&lt;/p>
&lt;p>&lt;img src="https://imesh.club/upload/2020/05/1589096005904.jpg?v=1589096014" alt="">&lt;/p>
&lt;p>The first visit to &lt;a href="http://authcode.imesh.club/personalinfo,">http://authcode.imesh.club/personalinfo,&lt;/a> the browser is automatically jumped to the vouch.imesh.club/login? interface, this jump is actually driven by NGINX&lt;/p>
&lt;p>After receiving it, vouch.imesh.club processes it and asks the browser to jump to the github.com/authorize interface. Since it has not logged in on github, github jumps to the /login interface to let the user log in.&lt;/p>
&lt;p>The login interface appears. After logging in, the authorization will be displayed. Clicking on the authorization will be redirected to vouch.imesh.club (the service address of oauth proxy), which actually returns the authorization code to the oauth poxy service.&lt;/p>
&lt;p>After clicking the authorization, the browser will continue to jump. The github implementation will have the following jump prompt, which is actually the browser to jump to the callback interface of vouch.imesh.club:&lt;/p>
&lt;p>&lt;img src="https://imesh.club/upload/2020/05/%E5%9B%BE%E7%89%87-3.png?v=1589095466" alt="">&lt;/p>
&lt;p>After the callback interface of vouch.imesh.club is accessed, it will drive vouch to initiate access_token acquisition on the server side. At this time, the browser cannot capture it. When vouch has been obtained on the server, it returns a 302 to the browser again. This 302 requires the browser to officially access the application address, and is accompanied by the relevant cookie to the client browser:&lt;/p>
&lt;p>&lt;img src="https://imesh.club/upload/2020/05/1589096907088.jpg?v=1589096914" alt="">&lt;/p>
&lt;p>Finally completed the visit:&lt;/p>
&lt;p>&lt;img src="https://imesh.club/upload/2020/05/1589097012011.jpg?v=1589097024" alt="">&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Use NGINX&amp;rsquo;s auth_request function, and through clever configuration to use oauth proxy to achieve the complete authentication process of oAuth, and pass relevant user information to NGINX to achieve access control and information processing. Except for the back-end, all applications need to develop code to implement oauth verification, so that enterprises can quickly use third-party accounts to control user access&lt;/p>
&lt;h2 id="follow-up">Follow up&lt;/h2>
&lt;p>In this practice, the authorization code mode of oAuth is adopted, and the external oauth proxy service is used. If you do not want to rely on external services and want to implement on pure NGINX, you can refer to the second part of this series .&lt;/p>
&lt;p>Check more oAuth posts of the series at my tech blog &lt;a href="https://imesh.club/?s=oauth">https://imesh.club/?s=oauth&lt;/a>&lt;/p></description></item><item><title>NGINX from zero to hero</title><link>http://linjing.io/talk/f5-nginx-traning-2020/</link><pubDate>Fri, 28 Feb 2020 16:13:30 +0000</pubDate><guid>http://linjing.io/talk/f5-nginx-traning-2020/</guid><description>&lt;!-- raw HTML omitted --></description></item></channel></rss>