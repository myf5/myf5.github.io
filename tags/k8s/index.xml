<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>k8s on Jing Lin‘s profile</title><link>http://linjing.io/tags/k8s/</link><description>Recent content in k8s on Jing Lin‘s profile</description><generator>Source Themes academia (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright &amp;copy; {year} linjing.io host on github, imesh.cloud host on netlify. CI/CD by github actions and netlify.Thanks bootcdn support for front libary CDN</copyright><lastBuildDate>Mon, 30 Jan 2023 00:00:00 +0000</lastBuildDate><atom:link href="http://linjing.io/tags/k8s/index.xml" rel="self" type="application/rss+xml"/><item><title>Project- BIG-IP K8S Gateway API</title><link>http://linjing.io/post/big-ip-gateway-api/</link><pubDate>Mon, 30 Jan 2023 00:00:00 +0000</pubDate><guid>http://linjing.io/post/big-ip-gateway-api/</guid><description>&lt;h2 id="name">Name&lt;/h2>
&lt;p>&lt;strong>Product Name&lt;/strong>: BIG-IP Kubernetes Gateway Controller&lt;/p>
&lt;p>&lt;strong>Code Name&lt;/strong>: bigip-kubernetes-gateway&lt;/p>
&lt;h2 id="description">Description&lt;/h2>
&lt;p>&lt;a href="https://gateway-api.sigs.k8s.io/">&lt;strong>Gateway API&lt;/strong>&lt;/a> is an open source project managed by the SIG-NETWORK community. It is a collection of resources that model service networking in Kubernetes. These resources - GatewayClass,Gateway, HTTPRoute, TCPRoute, Service, etc - aim to evolve Kubernetes service networking through expressive, extensible, and role-oriented interfaces that are implemented by many vendors and have broad industry support.&lt;/p>
&lt;p>&lt;strong>F5 BIG-IP kubernetes Gateway&lt;/strong> is an implementation of Kubernetes Gateway API. It uses F5 BIG-IP as the &lt;a href="https://gateway-api.sigs.k8s.io/implementations/">downstream implementation and integration&lt;/a>.&lt;/p>
&lt;p>See &lt;a href="https://gateway-api.f5se.io">https://gateway-api.f5se.io&lt;/a> for more information.&lt;/p>
&lt;p>Source code repository: &lt;a href="https://github.com/f5devcentral/bigip-kubernetes-gateway">https://github.com/f5devcentral/bigip-kubernetes-gateway&lt;/a>.&lt;/p>
&lt;h2 id="quick-view">Quick view&lt;/h2>
&lt;p>BIG-IP Kubernetes Gatewway enables the conversion of GatewayAPI resources to BIG-IP ADC capabilities automatically:&lt;/p>
&lt;p>&lt;a href="">&lt;img src="img/bigip-k8s-gateway-resource-mapping.png" alt="image">&lt;/a>&lt;/p>
&lt;h2 id="code">Code&lt;/h2>
&lt;p>&lt;a href="https://github.com/f5devcentral/bigip-kubernetes-gateway/tree/master">Github f5devcentral&lt;/a>&lt;/p>
&lt;h2 id="installation">Installation&lt;/h2>
&lt;p>The installation process is simple, see &lt;a href="https://gateway-api.f5se.io/quick-start/">https://gateway-api.f5se.io/quick-start/&lt;/a> for details.&lt;/p>
&lt;h2 id="usage">Usage&lt;/h2>
&lt;p>As the quick getstart, see &lt;a href="https://gateway-api.f5se.io/Use-Cases/simple-gateway/">https://gateway-api.f5se.io/Use-Cases/simple-gateway/&lt;/a>.&lt;/p>
&lt;h2 id="support">Support&lt;/h2>
&lt;p>For support, please open a &lt;a href="https://github.com/f5devcentral/bigip-kubernetes-gateway/issues">GitHub issue&lt;/a>. Note, the code in this repository is community supported and is not supported by F5.&lt;/p>
&lt;p>Maintenance and F5 Technical Support of this F5 code is provided only if the software (i) is unmodified; and (ii) has been marked as F5 Supported in SOL80012344, (&lt;a href="https://support.f5.com/csp/article/K80012344)">https://support.f5.com/csp/article/K80012344)&lt;/a>.&lt;/p>
&lt;p>Support will only be provided to customers who have an existing BIG-IP support contract associated with a valid BIG-IP serial number. For information about support policies, see &lt;a href="http://www.f5.com/about/guidelines-policies/">http://www.f5.com/about/guidelines-policies/&lt;/a> and &lt;a href="http://askf5.com">http://askf5.com&lt;/a>.&lt;/p>
&lt;h2 id="roadmap">Roadmap&lt;/h2>
&lt;p>The bigip-kubernetes-gateway versions are released on dockerhub as &lt;a href="https://hub.docker.com/r/f5devcentral/bigip-kubernetes-gateway/tags">Docker images&lt;/a>.&lt;/p>
&lt;p>For a list of supported Gateway API resources and features, see the &lt;a href="https://github.com/f5devcentral/bigip-kubernetes-gateway/blob/master/docs/gateway-api-compatibility.md">Gateway API Compatibility&lt;/a> doc. Check &lt;a href="https://github.com/kubernetes-sigs/gateway-api#status">here&lt;/a> for Gateway API&amp;rsquo;s latest status.&lt;/p>
&lt;h2 id="contributing">Contributing&lt;/h2>
&lt;p>We are open to contributions, if&lt;/p>
&lt;ul>
&lt;li>you are interested in participating in our project and&lt;/li>
&lt;li>you have experience in kubernetes and golang development,&lt;/li>
&lt;li>BIG-IP related applications or development experience is preferred.&lt;/li>
&lt;/ul>
&lt;h2 id="authors-and-acknowledgment">Authors and acknowledgment&lt;/h2>
&lt;p>Contributors for now：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/zongzw">zongzw f5zong&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/Niklaus-xie">Niklaus-xie&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/myf5">myf5 Jing Lin&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Why K8s Egress Traffic Policy Control is Critical to Security Architecture</title><link>http://linjing.io/post/why-k8s-egress-traffic-policy-control-is-critical-to-security-architecture/</link><pubDate>Sat, 06 Aug 2022 00:00:00 +0000</pubDate><guid>http://linjing.io/post/why-k8s-egress-traffic-policy-control-is-critical-to-security-architecture/</guid><description>&lt;h2 id="why-egress-traffic-policy-control">Why Egress Traffic Policy Control&lt;/h2>
&lt;p>According to the 2021 CNCF survey, the proportion of users who use Kubernetes (K8s) in the production environment has reached 59.77%, and the number of European users has reached 68.98%. More and more users are migrating production services to the Kubernetes environment. Gartner 2021 Hype Cycle for Cloud Security also shows that container and Kubernetes security is already in the &amp;ldquo;slope of Enlightenment&amp;rdquo; stage. This shows that protecting application services in Kubernetes is becoming more and more important.&lt;/p>
&lt;p>When we look at a large number of microservices running in Kubernetes, we can see that microservice security has the characteristics of typical micro-boundaries and need continuous security engineering. We need consider each microservice as a boundary and protect it, whether runtime, or north-south and east-west traffic. It is necessary for each microservice to start security considerations at the beginning of coding, and to move security to the left. Security methods, and policies should be adapted to developers and Kubernetes platform operators. It also requires the ability to gain insight into all traffic, collect all runtime logs, events and other data, analyze these data through a continuous security engineering system, aggregate rules and feed them back into security policy settings.&lt;/p>
&lt;p>&lt;img src="https://community.f5.com/t5/image/serverpage/image-id/18348i39592BD75177EE79/image-size/large?v=v2&amp;amp;px=999" alt="image-20220708090843383.png">&lt;/p>
&lt;p>The microservices in Kubernetes will not only run closed inside the cluster, it needs to access applications, databases, third-party API services, Internet services, etc. outside the cluster. The outgoing traffic may include the business traffic, open source component update traffic, or even traffic from the compromised application to the C2. Therefore, it is necessary to actively control the egress traffic of microservices in Kubernetes to ensure its security and compliance. Under the digital transformation driven by cloud-native architecture, enterprises will adopt a large number of open source technologies, which may be the easiest place to introduce security risks. Regardless of whether there is a clear open source whitelist mechanism, enterprises should pay enough attention to these open source technologies. These open source may take the initiative to visit the outside, we need control it well, and ensure safety.&lt;/p>
&lt;p>Managing outbound traffic policies in Kubernetes, a seemingly simple requirement, actually is not an easy task to do well. This article will analyze the challenges of Kubernetes outbound policies with you, analyze the advantages and disadvantages of current common solutions, and think about how enterprises should manage and control Kubernetes egress traffic policies.&lt;/p>
&lt;h2 id="existing-challenges">Existing challenges&lt;/h2>
&lt;h3 id="dynamic">Dynamic&lt;/h3>
&lt;p>From a technical point of view, this is the first challenge that exists. In a Kubernetes environment, the pods of the microservice will be highly dynamic and decentralized. IPs, network segments and physical locations will change over time. Therefore, it will be impossible to directly use IP and other characteristics for static policy setting. The strategy must rely on other abstract application labels, service names or namespaces, etc., and can dynamically perceive changes.&lt;/p>
&lt;h3 id="granularity">Granularity&lt;/h3>
&lt;p>In a traditional application environment, the management and control of an application&amp;rsquo;s outbound traffic policy generally only requires policy setting for a small number of deployments involved in the application. However, in the environment of microservices and containerization, an application may be composed of many microservices, and a microservice contains many pods. Different microservice units will require different outbound policy rules. For example, payment needs to connect to a third-party interface, while the comment service does not need to actively access third-party services. This means that the granularity of policy setting needs to be fine-grained to each microservice unit and ensure that all relevant pods are controlled. It can be seen that the policy control granularity is finer, the workload is larger, and the complexity is higher.&lt;/p>
&lt;h3 id="collaboration">Collaboration&lt;/h3>
&lt;p>When we want to deploy egress policies for different microservice, who should be responsible for this, the application development department? Application operation and maintenance department? PlatformOps division of Kubernetes? Or the security department? When we look at this with a security-left shift, it is obvious that the application department should consider whether his microservices need to actively access which external services. However, if the application developer is in charge, can the platform or security department just let it go? Obviously not, application developers set up security policies for the applications they are responsible for. Global basic security policies setting, and how to quickly remedy the wrong policies set by application developers, These still need to be taken care of by other teams. In order for the development department to practice the idea of security shift left, Platform ops team or the security department must provide friendly tools and integrate the security policy setting into the DevSecOps pipeline. If the tools or methods lead to a decline in development efficiency, developers will not be happy to use it. Therefore, this is not the independent work of a certain department, it requires the cooperation of multiple teams/departments to ensure safety.&lt;/p>
&lt;h3 id="data-driven">Data driven&lt;/h3>
&lt;p>As stated at the beginning of the article, security is a continuous engineering effort, meaning that any outgoing access behavior and events should be logged into the security engineering system for analysis to ensure adequate visibility and insight. Egress security control is not only a simple policy setting, but also requires the ability to output complete logs, behaviors, and events.&lt;/p>
&lt;h2 id="industry-solution-analysis">Industry solution analysis&lt;/h2>
&lt;p>Next, we will analyze the current industry solutions for egress policy control one by one. First, we will divide them into 6 categories of solutions, and then analyze them one by one:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>**Category **&lt;/th>
&lt;th>**Solutions or products **&lt;/th>
&lt;th>**Description **&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Platform based&lt;/td>
&lt;td>Kubernetes Network policy Openshift EgressIP Openshift Egress Router pod Openshift Egress Firewall Openshift EgressNetworkPolicy&lt;/td>
&lt;td>A specific feature of a platform provider&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CNI based&lt;/td>
&lt;td>Calico Egress pod Calico Enhanced Network policy Cilium Enhanced Network policy Kube-ovn&lt;/td>
&lt;td>The capability of CNI&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Service Mesh&lt;/td>
&lt;td>NGINX Service Mesh Istio&lt;/td>
&lt;td>A function of Service Mesh&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Micro segmentation&lt;/td>
&lt;td>PrismaNeu Vector&lt;/td>
&lt;td>From ZTA perspective or use enforcer container to control egress&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Fusion&lt;/td>
&lt;td>F5 CES(Container Egress Service) Fortinet&lt;/td>
&lt;td>Use k8s native method to integrate exist security assets to k8s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Others&lt;/td>
&lt;td>DNS interception Proxy pod&lt;/td>
&lt;td>Intercept coredns or use a proxy pod as forward proxy&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="platform-based">Platform based&lt;/h3>
&lt;p>The Network policy that comes with Kubernetes is the easiest way to think about egress security policy control. It is the native ability of K8s, which has natural affinity for developers or PlatformOps personnel, and can well adapt to the idea of security left shift. But Network policy needs CNI support. Some other disadvantages are:&lt;/p>
&lt;ul>
&lt;li>No cluster global policy capability, independent policies must be maintained under different namespaces&lt;/li>
&lt;li>No selection capability based on k8s svc name (can be changed to use pod label, but inflexible)&lt;/li>
&lt;li>No explicit rejection ability, through the isolation characteristics of the policy, and then impose a specific whitelist&lt;/li>
&lt;li>Rules without priority concept&lt;/li>
&lt;li>No dedicate external access rule type, external target services can only rely on broad ipblock&lt;/li>
&lt;li>pure four-layer, no seven-layer control capability&lt;/li>
&lt;li>No policy execution debugging capability&lt;/li>
&lt;li>No policy execution log&lt;/li>
&lt;li>The &amp;ldquo;isolation&amp;rdquo; feature of Networkpolicy makes maintenance very troublesome. For example, it only wants to control its access to the Internet, but because of isolation, additional maintenance is required All outbound (east-west) access of the pod within the cluster&lt;/li>
&lt;li>Does not solve the problem of k8s working with external security devices. Just imagine, after Network policy has made rule control, can an external security device open a default allow rule for the cluster?&lt;/li>
&lt;/ul>
&lt;p>Openshift has four features related to Egress, namely standard Network Policy, Egress IP, Egress Router, Egress Firewall and Egress NetworkPolicy.&lt;/p>
&lt;ul>
&lt;li>Network Policy, which is fully supported when Openshift uses OVN-Kubernetes as the CNI, while the traditional Openshift SDN CNI is only partially supported. It is no different from standard Kubernetes, and its advantages and disadvantages will not be analyzed here.&lt;/li>
&lt;li>EgressIP is a function to use deterministic source IP when Pods traffic leaves the cluster. When using Openshift SDN CNI, this function applies the Egress IP to the specified nodes as secondary IP and is used for SNAT. When using OVN-Kubernetes CNI, the snat rules are executed for specific pods through OVS. Using EgressIP itself is not a direct method for egress security policy control, but by specifying a certain source IP for different namespaces, some policies can be deployed on security devices outside the cluster to perform control. Obviously, this policy control method is relatively extensive, and cannot achieve the fine-grained granularity of different services. If the pods are scattered on different nodes, there will also be the problem that the outgoing cluster traffic of the pods must first traverse between different nodes, adding extra delay. In addition, the EgressIP must belong to the same network segment as the node&amp;rsquo;s main network address, and a node cannot have more than one EgressIP. EgressIP also does not support public cloud and Redhat Openstack Platform.&lt;/li>
&lt;li>Egress Router Pod, which is a special pod with two network interfaces, and uses MacVlan technology to directly connect one of the container network interface to the external network. All pods out-of-cluster traffic will pass through this pod. Different CNIs (SDN or OVN-Kubernetes) have different functions for the pod. Only redirection operations are supported under OVN-Kubernetes CNI. Generally speaking, this is not suitable for large-scale use. From the perspective of Egress security policy setting, it is still impossible to distinguish between different services, and the centralized Egress pod can easily become a performance bottleneck.&lt;/li>
&lt;li>EgressFirewall, which is actually a feature of OVN-Kubernetes. Allows setting outbound access rules for a project or namespace, which can be based on protocols, ports, IP, FQDN and other dimensions. The protocol only supports TCP, UDP, SCTP, and cannot support other protocol control. It only allows setting based on the namespace level. Only one rule file is allowed to be set in a namespace, and cannot set different rules for different services in the cluster. Also it limits each namespace/project to a maximum of 8000 rules. Observables or events are also not supported.&lt;/li>
&lt;li>Egress NetworkPolicy, similar to EgressFirewall, supports this CRD when using Openshift SDN CNI. But the Egress NetworkPolicy is more restrictive, for example, each namespace/project supports a maximum of 1000 rules, and the network policy or multitenant mode must be turned on.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>&amp;raquo; &lt;a href="https://community.f5.com/t5/technical-articles/what-you-should-know-about-k8s-egress-traffic-policy-control/ta-p/297895">Click here&lt;/a> for continue reading(2/2)&lt;/strong>&lt;/p></description></item><item><title>Project- Container Egress Service(CES)</title><link>http://linjing.io/post/f5-container-egress-service-ces/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>http://linjing.io/post/f5-container-egress-service-ces/</guid><description>&lt;h2 id="solution-architecture">Solution architecture&lt;/h2>
&lt;h3 id="components">Components&lt;/h3>
&lt;p>The CES solution includes the following components:&lt;/p>
&lt;ul>
&lt;li>CES controller: a container running in k8s. This component is the control plane, responsible for converting the outbound policies that deployed in k8s into the external data plane component(here is F5 AFM).&lt;/li>
&lt;li>F5 BIG-IP AFM: Data plane components running outside of k8s. Accept the configuration issued by the CES controller and execute specific access control rules, such as access control lists, bandwidth limiting, traffic programming, etc.&lt;/li>
&lt;li>CNI: CNI is a choice of the user environment itself and is not included in the CES plan. However, different CNIs will have different effects on the functions of the CES solution. Use &lt;a href="https://github.com/kubeovn/kube-ovn/">kube-ovn&lt;/a> CNI to get the full functionality of CES.&lt;/li>
&lt;/ul>
&lt;h3 id="architecture-diagram">Architecture diagram&lt;/h3>
&lt;p>&lt;img src="img/high-level-arch.jpg" alt="high-level-arch">&lt;/p>
&lt;h3 id="policy-scope-and-role">Policy scope and role&lt;/h3>
&lt;p>CES provides three policy scopes &lt;code>cluster global&lt;/code> &lt;code>namespace&lt;/code> and &lt;code>service&lt;/code>. Its meaning and user role relationship are as follows:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>scope&lt;/th>
&lt;th>meaning&lt;/th>
&lt;th>Adaptation role&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Cluster global&lt;/td>
&lt;td>It is the global level policy of the cluster, which is used to control the general and overall access control of the cluster. For example, the cluster&amp;rsquo;s access to basic public services such as NTP and DNS of the enterprise. The scope policy is applied to the outbound access control of all services in the cluster.&lt;/td>
&lt;td>Cluster administrator, Security team&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>namespace&lt;/td>
&lt;td>This policy level is effective for a single namespace or project. It is used to control the access of all services in specific NS or project to access the services out of the cluster. Policies in different namespaces or projects do not affect each other. *This function requires the support of CNI.&lt;/td>
&lt;td>Project team, application operation team&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>service level&lt;/td>
&lt;td>The policy control the k8s servcies to the external services. Only valid for specific services. So if the CNI can not support namespace level policy, set svc level policy is an alternative way.&lt;/td>
&lt;td>Project team, application operation, microservice owner&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="tenant-isolation">Tenant isolation&lt;/h3>
&lt;p>The CES solution supports strong isolation of &lt;code>network + namespace&lt;/code>. Supports the administrative isolation of configuration objects on the data plane, and also supports strict traffic isolation at the network level. Support different namespaces to use overlapping CIDR.&lt;/p>
&lt;pre>&lt;code>Need CNI support network isolation. For example kube-ovn's per ns subnet
&lt;/code>&lt;/pre>&lt;h2 id="solution-value">Solution value&lt;/h2>
&lt;h3 id="challenges-solved">Challenges solved&lt;/h3>
&lt;ul>
&lt;li>High-frequency changes in outbound traffic caused by container IP dynamics&lt;/li>
&lt;li>Different role groups have different requirements for the scope setting of the policy, and the policy needs to match the role in multiple dimensions&lt;/li>
&lt;li>Dynamic bandwidth limit requirements for outbound traffic&lt;/li>
&lt;li>Protocol in-depth security inspection requirements&lt;/li>
&lt;li>Advanced requirements for flow programmable based on access control events&lt;/li>
&lt;li>Visualization requirements for outbound traffic&lt;/li>
&lt;/ul>
&lt;h3 id="provided-capabilities">Provided capabilities&lt;/h3>
&lt;ul>
&lt;li>Dynamic IP ACL control with Cluster/Pod/NS granularity&lt;/li>
&lt;li>Cluster/Pod/NS granular FQDN ACL control&lt;/li>
&lt;li>Time-based access control&lt;/li>
&lt;li>Matched flow event trigger and programmable&lt;/li>
&lt;li>Matched traffic redirection&lt;/li>
&lt;li>Protocol security and compliance testing&lt;/li>
&lt;li>IP intelligence&lt;/li>
&lt;li>Traffic matching log&lt;/li>
&lt;li>Traffic matching visualization report&lt;/li>
&lt;li>Protocol detection visual report&lt;/li>
&lt;li>TCP/IP Errors report&lt;/li>
&lt;li>NAT control and logging&lt;/li>
&lt;li>Data flow visualization tracking&lt;/li>
&lt;li>Visual simulation of access rules&lt;/li>
&lt;li>Transparent detection mode&lt;/li>
&lt;li>High-speed log outgoing&lt;/li>
&lt;/ul>
&lt;pre>&lt;code>Partial functions will evolve along with version iterations
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Next step:&lt;/strong>&lt;/p>
&lt;p>&lt;a href="https://github.com/f5devcentral/container-egress-service/wiki/EN_2_CES_Installation">Understanding CES installation&lt;/a>&lt;/p></description></item></channel></rss>